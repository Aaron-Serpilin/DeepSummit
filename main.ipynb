{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Framework for Predicting Himalayan Summit Success \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "The following section of the notebook covers all relevant imports required for the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is only for Google Collab whenever wanting to perform runs there\n",
    "# !git clone https://github.com/Aaron-Serpilin/DeepSummit.git\n",
    "# !pip install --upgrade pip\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.6.0\n",
      "torchvision version: 0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronserpilin/anaconda3/envs/Python3_Env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlextend version: 0.23.4\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[0]) >= 2, \"torch version should be 2.+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 15, \"torchvision version should be 0.15+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not correct. Installing correct versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "    print(\"[INFO] Couldn't find matplotlib...installing it\")\n",
    "    !pip install -q matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except:\n",
    "    print(f\"[INFO] Couldnt't find tqdm... installing it \")\n",
    "    !pip install tqdm\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except ImportError:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "try:\n",
    "    from dbfread import DBF\n",
    "except ImportError:\n",
    "    print(\"[INFO] Coudln't find dbfread...installing it\")\n",
    "    !pip install -q dbfread\n",
    "    from dbfread import DBF\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find tensorboard... installing it.\")\n",
    "    !pip install -q tensorboard\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "try:\n",
    "    import torchmetrics, mlxtend\n",
    "    print(f\"mlextend version: {mlxtend.__version__}\")\n",
    "    assert int(mlxtend.__version__.split(\".\")[1]) >- 19\n",
    "except:\n",
    "    !pip install -q torchmetrics -U mlxtend\n",
    "    import torchmetrics, mlxtend\n",
    "    print(f\"mlextend version: {mlxtend.__version__}\")\n",
    "\n",
    "try:\n",
    "    import cdsapi\n",
    "except ImportError:\n",
    "    print(\"[INFO] Coudldn't find cdsapi...installing it.\")\n",
    "    !pip install -q cdsapi\n",
    "    import cdsapi\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    print(\"[INFO] Couldn't find pandas... installing it\")\n",
    "    !pip install -q pandas\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    from einops import rearrange, repeat\n",
    "except ImportError:\n",
    "    print(\"[INFO] Couldn't find einops... installing it\")\n",
    "    !pip install -q einops\n",
    "    from einops import rearrange, repeat\n",
    "\n",
    "try:\n",
    "    import pygrib\n",
    "except ImportError:\n",
    "    print(\"[INFO] Couldn't find pygrib... installing it\")\n",
    "    !pip install -q pygrib\n",
    "    import pygrib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Himalayan Data Setup\n",
    "\n",
    "The following section covers the retrieval and processing of the tabular data from the Himalayan Database that will use the SAINT architecture to carry out inference. The raw data was obtained through the `get_data.py` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We adjust the PYTHONPATH to keep absolute imports\n",
    "import sys\n",
    "sys.path.append(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "himalayan_train_dir = Path(\"data/himalayas_data/train\")\n",
    "himalayan_val_dir = Path(\"data/himalayas_data/val\")\n",
    "himalayan_test_dir = Path(\"data/himalayas_data/test\")\n",
    "\n",
    "himalayan_train_file = himalayan_train_dir / \"train.csv\"\n",
    "himalayan_val_file = himalayan_val_dir / \"val.csv\"\n",
    "himalayan_test_file = himalayan_test_dir / \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows:\n",
      "  SEX      CITIZEN      STATUS  MO2USED  MROUTE1  SEASON  O2USED  CALCAGE  \\\n",
      "0   M        Japan     Climber     True        1       3    True       49   \n",
      "1   M        Spain     Climber    False        1       3   False       54   \n",
      "2   F  Switzerland     Climber    False        1       3    True       25   \n",
      "3   M        Nepal  H-A Worker     True        1       1    True       31   \n",
      "4   M          USA     Climber     True        1       1    True       60   \n",
      "5   M      Bahrain     Climber    False        1       3    True       34   \n",
      "6   M        Chile     Climber     True        1       1    True       42   \n",
      "7   F        Japan      Leader     True        0       4    True       43   \n",
      "8   M        India     Climber     True        1       1    True       48   \n",
      "9   M      S Korea     Climber    False        0       3   False       29   \n",
      "\n",
      "   HEIGHTM  MDEATHS  HDEATHS  SMTMEMBERS  SMTHIRED  Target  \n",
      "0     8163        0        0          14        11       1  \n",
      "1     8163        0        0           1         1       0  \n",
      "2     8485        0        0           1         0       0  \n",
      "3     8849        0        0           0         9       1  \n",
      "4     8188        0        0           6         3       1  \n",
      "5     8163        0        0          13        43       1  \n",
      "6     8849        0        0           0         0       0  \n",
      "7     8849        0        0           0         0       0  \n",
      "8     8849        0        0           8        11       1  \n",
      "9     8188        0        0           5         1       0  \n",
      "First training instance:\n",
      "SEX                 M\n",
      "CITIZEN         Japan\n",
      "STATUS        Climber\n",
      "MO2USED          True\n",
      "MROUTE1             1\n",
      "SEASON              3\n",
      "O2USED           True\n",
      "CALCAGE            49\n",
      "HEIGHTM          8163\n",
      "MDEATHS             0\n",
      "HDEATHS             0\n",
      "SMTMEMBERS         14\n",
      "SMTHIRED           11\n",
      "Target              1\n",
      "Name: 0, dtype: object\n",
      "Instance shape:\n",
      "(14,)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(himalayan_train_file)\n",
    "\n",
    "print(f\"First 10 rows:\\n{df_train.head(10)}\")\n",
    "print(f\"First training instance:\\n{df_train.iloc[0]}\")\n",
    "print(f\"Instance shape:\\n{df_train.iloc[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_himalayan_data()` function defined within the `tab_data_setup` file transforms the raw data from .DBF to .csv, filters the relevant features, filters the relevant peaks, develops a data frame with this data, and then creates the corresponding train, val, and test splits by using the `create_dataloaders()` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training set saved to: data/himalayas_data/train/train.csv\n",
      "[INFO] Validation set saved to: data/himalayas_data/val/val.csv\n",
      "[INFO] Test set saved to: data/himalayas_data/test/test.csv\n",
      "[INFO] Himalaya Data has already been processed\n"
     ]
    }
   ],
   "source": [
    "from src.tab_transformer.tab_data_setup import load_himalayan_data\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = load_himalayan_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SAINT Model instantiation\n",
    "\n",
    "The following section instantiates the SAINT model architecture based on the hyperparameter selection defined in the relevant paper. This is broken down in the `tab_breakdown.ipynb` file where the core equations and backbone of the architecture is explained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical instance: tensor([  0,   1, 147,  60,   0,   1,   2,   0])\n",
      "Categorical instance shape: torch.Size([8])\n",
      "\n",
      "Continuous instance: tensor([ 0.2272, -1.1185, -0.2801, -0.1664, -0.8072, -0.6175])\n",
      "Continuous instance shape: torch.Size([6])\n",
      "\n",
      "Label instance: 0\n",
      "Label instance shape: torch.Size([])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_batch, cont_batch, label_batch, cat_mask_batch, cont_mask_batch = next(iter(train_dataloader))\n",
    "\n",
    "# First instance\n",
    "cat_instance = cat_batch[0]\n",
    "cont_instance = cont_batch[0]\n",
    "label_instance = label_batch[0]\n",
    "\n",
    "print(f\"Categorical instance: {cat_instance}\\nCategorical instance shape: {cat_instance.shape}\\n\")\n",
    "print(f\"Continuous instance: {cont_instance}\\nContinuous instance shape: {cont_instance.shape}\\n\")\n",
    "print(f\"Label instance: {label_instance}\\nLabel instance shape: {label_instance.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tab_transformer.tab_model import SAINT\n",
    "import numpy as np\n",
    "\n",
    "categorical_columns = ['SEX', 'CITIZEN', 'STATUS', 'MO2USED', 'MROUTE1', 'SEASON', 'O2USED']\n",
    "continuous_columns = ['CALCAGE', 'HEIGHTM', 'MDEATHS', 'HDEATHS', 'SMTMEMBERS', 'SMTHIRED']\n",
    "\n",
    "# Returns the amount of unique values per categorical column\n",
    "cat_dims = [len(np.unique(df_train[col])) for col in categorical_columns]\n",
    "\n",
    "# Hyperparameter selection based on default original architecture instantiation\n",
    "saint = SAINT(\n",
    "    categories = tuple(cat_dims), \n",
    "    num_continuous = len(continuous_columns),                \n",
    "    dim = 32,                           \n",
    "    dim_out = 1,                       \n",
    "    depth = 6,                       \n",
    "    heads = 8,  \n",
    "    num_special_tokens=1,                      \n",
    "    attn_dropout = 0.1,             \n",
    "    ff_dropout = 0.1,                  \n",
    "    mlp_hidden_mults = (4, 2),       \n",
    "    cont_embeddings = 'MLP',\n",
    "    attentiontype = 'colrow',\n",
    "    final_mlp_style = 'sep',\n",
    "    y_dim = 2 # Binary classification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SAINT Model Training\n",
    "\n",
    "The following section trains the SAINT model with the `TabularDataset` class while saving the corresponding files in the `runs` directory using the `SummaryWriter()` setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0005842532558613513, 0.9128137550200803)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.tab_transformer.tab_train import train_step, test_step\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.AdamW(saint.parameters(),lr=0.0001, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "\n",
    "train_step(model=saint,\n",
    "           dataloader=train_dataloader,\n",
    "           loss_fn=loss_fn,\n",
    "           optimizer=optimizer,\n",
    "           device=device\n",
    ")\n",
    "\n",
    "val_step = test_step(model=saint,\n",
    "                     dataloader=val_dataloader,\n",
    "                     loss_fn=loss_fn,\n",
    "                     device=device)\n",
    "\n",
    "val_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_writer(experiment_name: str,\n",
    "                  model_name: str,\n",
    "                  extra: str=None) -> torch.utils.tensorboard.writer.SummaryWriter():\n",
    "\n",
    "                  from datetime import datetime\n",
    "                  import os\n",
    "\n",
    "                  timestamp = datetime.now().strftime(\"%Y-%m-%d--%H:%M:%S\")\n",
    "\n",
    "                  if extra:\n",
    "                       log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra) # Create the log directory path\n",
    "                  else:\n",
    "                       log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name) # Create the log directory path\n",
    "\n",
    "                  print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}\")\n",
    "                  return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter, saving to: runs/2025-04-23--17:33:24/first_run/saint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [05:01<45:11, 301.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.1886 | train_acc: 0.9252 | test_loss: 0.0008 | test_acc: 0.9183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [09:49<39:08, 293.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.1860 | train_acc: 0.9254 | test_loss: 0.0007 | test_acc: 0.9170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [14:13<32:40, 280.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 0.1818 | train_acc: 0.9277 | test_loss: 0.0008 | test_acc: 0.9189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [18:50<27:53, 279.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 0.1781 | train_acc: 0.9295 | test_loss: 0.0009 | test_acc: 0.9157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [23:32<23:19, 279.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 0.1766 | train_acc: 0.9283 | test_loss: 0.0009 | test_acc: 0.9202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [28:44<19:22, 290.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss: 0.1738 | train_acc: 0.9305 | test_loss: 0.0007 | test_acc: 0.9172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [33:21<14:19, 286.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | train_loss: 0.1693 | train_acc: 0.9322 | test_loss: 0.0008 | test_acc: 0.9192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [37:50<09:21, 280.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | train_loss: 0.1666 | train_acc: 0.9328 | test_loss: 0.0005 | test_acc: 0.9168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [42:25<04:39, 279.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | train_loss: 0.1633 | train_acc: 0.9338 | test_loss: 0.0006 | test_acc: 0.9140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [47:06<00:00, 282.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | train_loss: 0.1603 | train_acc: 0.9356 | test_loss: 0.0007 | test_acc: 0.9147\n",
      "CPU times: user 1h 37min 16s, sys: 27min 29s, total: 2h 4min 45s\n",
      "Wall time: 47min 6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.18862296600337983,\n",
       "  0.18602193555800267,\n",
       "  0.181752410843542,\n",
       "  0.17810713039873818,\n",
       "  0.176648545670828,\n",
       "  0.17383407515961766,\n",
       "  0.16925857581215045,\n",
       "  0.16661058767160108,\n",
       "  0.1633284955933488,\n",
       "  0.1602522330709273],\n",
       " 'train_acc': [0.9252192593905032,\n",
       "  0.9253576807228916,\n",
       "  0.927734375,\n",
       "  0.9295310838944011,\n",
       "  0.9282783708362864,\n",
       "  0.9304931121545005,\n",
       "  0.9321583207831325,\n",
       "  0.9328172063253012,\n",
       "  0.9338055346385542,\n",
       "  0.9356174698795181],\n",
       " 'val_loss': [0.0005662141435117606,\n",
       "  0.0005597891847053206,\n",
       "  0.0005514946687652404,\n",
       "  0.0005323321794170931,\n",
       "  0.0006148527903729174,\n",
       "  0.0003084733364093735,\n",
       "  0.0003268823296908873,\n",
       "  0.0010838294962802566,\n",
       "  0.0005407305097723581,\n",
       "  0.00048101495906531093],\n",
       " 'val_acc': [0.9141315261044177,\n",
       "  0.9120732931726908,\n",
       "  0.9167796184738956,\n",
       "  0.9060491967871486,\n",
       "  0.9175326305220883,\n",
       "  0.9184864457831325,\n",
       "  0.9179216867469879,\n",
       "  0.9073544176706827,\n",
       "  0.9109437751004016,\n",
       "  0.9147088353413655],\n",
       " 'test_loss': [0.0007675058511366327,\n",
       "  0.0006990632019847272,\n",
       "  0.0008218576570591295,\n",
       "  0.0009248704615845738,\n",
       "  0.0008618514760431037,\n",
       "  0.0006728816373520587,\n",
       "  0.0007738303168710456,\n",
       "  0.0005470019237822797,\n",
       "  0.0006315280425261302,\n",
       "  0.0007081661059195737],\n",
       " 'test_acc': [0.9182860474154684,\n",
       "  0.916968276331131,\n",
       "  0.9188508064516129,\n",
       "  0.9156505052467936,\n",
       "  0.9201685775359503,\n",
       "  0.9171565293431791,\n",
       "  0.9192273124757093,\n",
       "  0.9167860959968908,\n",
       "  0.9139562281383599,\n",
       "  0.9147092401865526]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from helper_functions import set_seeds\n",
    "from src.tab_transformer.tab_train import train\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "\n",
    "results = train(model=saint,\n",
    "      train_dataloader=train_dataloader,\n",
    "      val_dataloader=val_dataloader,\n",
    "      test_dataloader=test_dataloader,\n",
    "      optimizer=optimizer,\n",
    "      loss_fn=loss_fn,\n",
    "      epochs=10,\n",
    "      writer=create_writer(experiment_name=\"first_run\",\n",
    "                                   model_name=\"saint\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a `test_acc` of ~92%, the SAINT Model architecture achieves better results than the benchmark Regression model from the Julia project. Therefore, prior to fine tuning the model, and without the incorporation of contextual meteorological data, the model already has a better performance than the baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3a7cc138ff9586d0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3a7cc138ff9586d0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Era5 Data Setup\n",
    "\n",
    "The following section covers the data preparation of the era5 dataset. The raw data was obtained through the `get_data.py` file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire data setup from the era5 dataset is condensed into the `load_era5_data` function, which does multiple things. \n",
    "\n",
    "Firstly, it uses`file_to_grib()` to convert all the raw era5 files into .grib files, which facilitate it for processing afterwards. \n",
    "\n",
    "This is followed by `get_variable_mapping()` which obtains the corresponding mapping of the internal grib abbreviations and the actual features from the era5 dataset. This is used to then filter out the desired features when we transform the .grib file into a .csv file. \n",
    "\n",
    "For convenience, we can run `get_variable_mapping()` each time, but in this study case the mapping doesn't change every time. Therefore, we can store the result of the first run in a variable and then merely refer to it, which saves up ~3 minutes of runtime.  \n",
    "\n",
    "This mapping is this passed as a parameter to `process_grib_to_csv()` to filter out the desired features and convert the .grib files to .csv, storing them in `data/era5_data/instances/raw_instances`.\n",
    "\n",
    "Afterwards, due to the API request limitations, the original requests in `get_data.py` were done in batches of 5 years, leading to each mountain to have 17 different weather data files. Therefore, in order to condense them, we then use `merge_daily_instances()` which condenses all of these files into a single .csv file in `data/era5_data/instances/merged_instances`. \n",
    "\n",
    "The final step of the function is to then use `build_event_instances()` to create the final ML table by the weather and tabular data are matched based on their `date` and `peakid`. It then creates the instances with the target variable from the tabular dataset, and also adds the relevant 7-day context window to analyze the weather trends that affect summit probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.met_transformer.met_data_setup import load_era5_data\n",
    "\n",
    "ml_table = load_era5_data()\n",
    "instances_output_path = Path('data/era5_data/instances')\n",
    "instances_output_path.mkdir(parents=True, exist_ok=True)\n",
    "ml_table_output_file = instances_output_path / 'ml_table.csv'\n",
    "ml_table.to_csv(ml_table_output_file, index=False)\n",
    "print(f\"Wrote {len(ml_table)} event instances to {ml_table_output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
