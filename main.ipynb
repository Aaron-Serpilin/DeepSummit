{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Framework for Predicting Himalayan Summit Success \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is only for Google Collab whenever wanting to perform runs there\n",
    "# !git clone https://github.com/Aaron-Serpilin/DeepSummit.git\n",
    "# !pip install --upgrade pip\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.6.0\n",
      "torchvision version: 0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronserpilin/anaconda3/envs/Python3_Env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlextend version: 0.23.4\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[0]) >= 2, \"torch version should be 2.+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 15, \"torchvision version should be 0.15+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not correct. Installing correct versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "    print(\"[INFO] Couldn't find matplotlib...installing it\")\n",
    "    !pip install -q matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except:\n",
    "    print(f\"[INFO] Couldnt't find tqdm... installing it \")\n",
    "    !pip install tqdm\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except ImportError:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "try:\n",
    "    from dbfread import DBF\n",
    "except ImportError:\n",
    "    print(\"[INFO] Coudln't find dbfread...installing it\")\n",
    "    !pip install -q dbfread\n",
    "    from dbfread import DBF\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find tensorboard... installing it.\")\n",
    "    !pip install -q tensorboard\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "try:\n",
    "    import torchmetrics, mlxtend\n",
    "    print(f\"mlextend version: {mlxtend.__version__}\")\n",
    "    assert int(mlxtend.__version__.split(\".\")[1]) >- 19\n",
    "except:\n",
    "    !pip install -q torchmetrics -U mlxtend\n",
    "    import torchmetrics, mlxtend\n",
    "    print(f\"mlextend version: {mlxtend.__version__}\")\n",
    "\n",
    "try:\n",
    "    import cdsapi\n",
    "except ImportError:\n",
    "    print(\"[INFO] Coudldn't find cdsapi...installing it.\")\n",
    "    !pip install -q cdsapi\n",
    "    import cdsapi\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    print(\"[INFO] Couldn't find pandas... installing it\")\n",
    "    !pip install -q pandas\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    from einops import rearrange, repeat\n",
    "except ImportError:\n",
    "    print(\"[INFO] Couldn't find einops... installing it\")\n",
    "    !pip install -q einops\n",
    "    from einops import rearrange, repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We adjust the PYTHONPATH to keep absolute imports\n",
    "import sys\n",
    "sys.path.append(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "himalayan_train_dir = Path(\"data/himalayas_data/train\")\n",
    "himalayan_val_dir = Path(\"data/himalayas_data/val\")\n",
    "himalayan_test_dir = Path(\"data/himalayas_data/test\")\n",
    "\n",
    "himalayan_train_file = himalayan_train_dir / \"train.csv\"\n",
    "himalayan_val_file = himalayan_val_dir / \"val.csv\"\n",
    "himalayan_test_file = himalayan_test_dir / \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows:\n",
      "  SEX      CITIZEN      STATUS  MO2USED  MROUTE1  SEASON  O2USED  CALCAGE  \\\n",
      "0   M        Japan     Climber     True        1       3    True       49   \n",
      "1   M        Spain     Climber    False        1       3   False       54   \n",
      "2   F  Switzerland     Climber    False        1       3    True       25   \n",
      "3   M        Nepal  H-A Worker     True        1       1    True       31   \n",
      "4   M          USA     Climber     True        1       1    True       60   \n",
      "5   M      Bahrain     Climber    False        1       3    True       34   \n",
      "6   M        Chile     Climber     True        1       1    True       42   \n",
      "7   F        Japan      Leader     True        0       4    True       43   \n",
      "8   M        India     Climber     True        1       1    True       48   \n",
      "9   M      S Korea     Climber    False        0       3   False       29   \n",
      "\n",
      "   HEIGHTM  MDEATHS  HDEATHS  SMTMEMBERS  SMTHIRED  Target  \n",
      "0     8163        0        0          14        11       1  \n",
      "1     8163        0        0           1         1       0  \n",
      "2     8485        0        0           1         0       0  \n",
      "3     8849        0        0           0         9       1  \n",
      "4     8188        0        0           6         3       1  \n",
      "5     8163        0        0          13        43       1  \n",
      "6     8849        0        0           0         0       0  \n",
      "7     8849        0        0           0         0       0  \n",
      "8     8849        0        0           8        11       1  \n",
      "9     8188        0        0           5         1       0  \n",
      "First training instance:\n",
      "SEX                 M\n",
      "CITIZEN         Japan\n",
      "STATUS        Climber\n",
      "MO2USED          True\n",
      "MROUTE1             1\n",
      "SEASON              3\n",
      "O2USED           True\n",
      "CALCAGE            49\n",
      "HEIGHTM          8163\n",
      "MDEATHS             0\n",
      "HDEATHS             0\n",
      "SMTMEMBERS         14\n",
      "SMTHIRED           11\n",
      "Target              1\n",
      "Name: 0, dtype: object\n",
      "Instance shape:\n",
      "(14,)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(himalayan_train_file)\n",
    "\n",
    "print(f\"First 10 rows:\\n{df_train.head(10)}\")\n",
    "print(f\"First training instance:\\n{df_train.iloc[0]}\")\n",
    "print(f\"Instance shape:\\n{df_train.iloc[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training set saved to: data/himalayas_data/train/train.csv\n",
      "[INFO] Validation set saved to: data/himalayas_data/val/val.csv\n",
      "[INFO] Test set saved to: data/himalayas_data/test/test.csv\n",
      "[INFO] Himalaya Data has already been processed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x30fe41fd0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x30fe7a9d0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1431e6250>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data_setup import train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical instance: tensor([  0,   1, 147,  60,   0,   1,   2,   0])\n",
      "Categorical instance shape: torch.Size([8])\n",
      "\n",
      "Continuous instance: tensor([ 0.2272, -1.1185, -0.2801, -0.1664, -0.8072, -0.6175])\n",
      "Continuous instance shape: torch.Size([6])\n",
      "\n",
      "Label instance: 0\n",
      "Label instance shape: torch.Size([])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_batch, cont_batch, label_batch, cat_mask_batch, cont_mask_batch = next(iter(train_dataloader))\n",
    "\n",
    "# First instance\n",
    "cat_instance = cat_batch[0]\n",
    "cont_instance = cont_batch[0]\n",
    "label_instance = label_batch[0]\n",
    "\n",
    "print(f\"Categorical instance: {cat_instance}\\nCategorical instance shape: {cat_instance.shape}\\n\")\n",
    "print(f\"Continuous instance: {cont_instance}\\nContinuous instance shape: {cont_instance.shape}\\n\")\n",
    "print(f\"Label instance: {label_instance}\\nLabel instance shape: {label_instance.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tab_transformer.tab_model import SAINT\n",
    "from src.data_setup import continuous_columns, categorical_columns\n",
    "import numpy as np\n",
    "\n",
    "# Returns the amount of unique values per categorical column\n",
    "cat_dims = [len(np.unique(df_train[col])) for col in categorical_columns]\n",
    "\n",
    "# Hyperparameter selection based on default original architecture instantiation\n",
    "saint = SAINT(\n",
    "    categories = tuple(cat_dims), \n",
    "    num_continuous = len(continuous_columns),                \n",
    "    dim = 32,                           \n",
    "    dim_out = 1,                       \n",
    "    depth = 6,                       \n",
    "    heads = 8,  \n",
    "    num_special_tokens=1,                       \n",
    "    attn_dropout = 0.1,             \n",
    "    ff_dropout = 0.1,                  \n",
    "    mlp_hidden_mults = (4, 2),       \n",
    "    cont_embeddings = 'MLP',\n",
    "    attentiontype = 'colrow',\n",
    "    final_mlp_style = 'sep',\n",
    "    y_dim = 2 # Binary classification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.004248567015291697, 0.4513805220883534)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.tab_transformer.tab_train import train_step, test_step\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.AdamW(saint.parameters(),lr=0.0001, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "\n",
    "train_step(model=saint,\n",
    "           dataloader=train_dataloader,\n",
    "           loss_fn=loss_fn,\n",
    "           optimizer=optimizer,\n",
    "           device=device\n",
    ")\n",
    "\n",
    "val_step = test_step(model=saint,\n",
    "                     dataloader=val_dataloader,\n",
    "                     loss_fn=loss_fn,\n",
    "                     device=device)\n",
    "\n",
    "val_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_writer(experiment_name: str,\n",
    "                  model_name: str,\n",
    "                  extra: str=None) -> torch.utils.tensorboard.writer.SummaryWriter():\n",
    "\n",
    "                  from datetime import datetime\n",
    "                  import os\n",
    "\n",
    "                  timestamp = datetime.now().strftime(\"%Y-%m-%d--%H:%M:%S\")\n",
    "\n",
    "                  if extra:\n",
    "                       log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra) # Create the log directory path\n",
    "                  else:\n",
    "                       log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name) # Create the log directory path\n",
    "\n",
    "                  print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}\")\n",
    "                  return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from helper_functions import set_seeds\n",
    "from src.tab_transformer.tab_train import train\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "\n",
    "train(model=saint,\n",
    "      train_dataloader=train_dataloader,\n",
    "      val_dataloader=val_dataloader,\n",
    "      test_dataloader=test_dataloader,\n",
    "      optimizer=optimizer,\n",
    "      loss_fn=loss_fn,\n",
    "      epochs=10,\n",
    "      writer=create_writer(experiment_name=\"first_run\",\n",
    "                                   model_name=\"saint\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
