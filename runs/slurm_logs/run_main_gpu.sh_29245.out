torch version: 2.0.1
torchvision version: 0.15.2
mlxtend version: 0.23.4
numpy version: 2.0.1
Device is: cpu

[INFO] Data splits already exist under data/himalayas_data. Skipping split.
Tabular train dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f5e1c653a60>
Tabular val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f5e1bd01030>
Tabular test dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f5e1bd010f0>

[INFO] Created SummaryWriter, saving to: runs/saint_runs/epochs_50_lr_1e-6_depth_2_dropout_5e-1/2025-06-11--16:00:58
Epoch: 1 | train_loss: 0.4976 | train_acc: 0.7951 | val_loss: 0.3751 | val_acc: 0.8462 | test_loss: 0.3580 | test_acc: 0.8492
Epoch: 2 | train_loss: 0.3025 | train_acc: 0.8713 | val_loss: 0.2965 | val_acc: 0.8786 | test_loss: 0.2822 | test_acc: 0.8821
Epoch: 3 | train_loss: 0.2621 | train_acc: 0.8913 | val_loss: 0.2776 | val_acc: 0.8808 | test_loss: 0.2624 | test_acc: 0.8919
Epoch: 4 | train_loss: 0.2477 | train_acc: 0.8991 | val_loss: 0.2647 | val_acc: 0.8897 | test_loss: 0.2523 | test_acc: 0.8989
Epoch: 5 | train_loss: 0.2383 | train_acc: 0.9038 | val_loss: 0.2546 | val_acc: 0.8949 | test_loss: 0.2513 | test_acc: 0.9047
Epoch: 6 | train_loss: 0.2290 | train_acc: 0.9085 | val_loss: 0.2511 | val_acc: 0.8912 | test_loss: 0.2370 | test_acc: 0.9051
Epoch: 7 | train_loss: 0.2206 | train_acc: 0.9129 | val_loss: 0.2430 | val_acc: 0.8970 | test_loss: 0.2366 | test_acc: 0.9060
Epoch: 8 | train_loss: 0.2147 | train_acc: 0.9150 | val_loss: 0.2413 | val_acc: 0.8957 | test_loss: 0.2284 | test_acc: 0.9070
Epoch: 9 | train_loss: 0.2106 | train_acc: 0.9164 | val_loss: 0.2411 | val_acc: 0.8959 | test_loss: 0.2241 | test_acc: 0.9085
Epoch: 10 | train_loss: 0.2063 | train_acc: 0.9179 | val_loss: 0.2351 | val_acc: 0.9008 | test_loss: 0.2234 | test_acc: 0.9083
Epoch: 11 | train_loss: 0.2038 | train_acc: 0.9188 | val_loss: 0.2315 | val_acc: 0.9043 | test_loss: 0.2249 | test_acc: 0.9089
Epoch: 12 | train_loss: 0.2026 | train_acc: 0.9191 | val_loss: 0.2334 | val_acc: 0.9006 | test_loss: 0.2179 | test_acc: 0.9087
Epoch: 13 | train_loss: 0.2001 | train_acc: 0.9202 | val_loss: 0.2339 | val_acc: 0.9017 | test_loss: 0.2184 | test_acc: 0.9089
Epoch: 14 | train_loss: 0.1987 | train_acc: 0.9205 | val_loss: 0.2290 | val_acc: 0.9038 | test_loss: 0.2186 | test_acc: 0.9107
Epoch: 15 | train_loss: 0.1979 | train_acc: 0.9209 | val_loss: 0.2269 | val_acc: 0.9057 | test_loss: 0.2202 | test_acc: 0.9094
Epoch: 16 | train_loss: 0.1960 | train_acc: 0.9211 | val_loss: 0.2323 | val_acc: 0.9015 | test_loss: 0.2142 | test_acc: 0.9107
Epoch: 17 | train_loss: 0.1946 | train_acc: 0.9223 | val_loss: 0.2256 | val_acc: 0.9062 | test_loss: 0.2179 | test_acc: 0.9104
Epoch: 18 | train_loss: 0.1935 | train_acc: 0.9222 | val_loss: 0.2288 | val_acc: 0.9038 | test_loss: 0.2144 | test_acc: 0.9117
Epoch: 19 | train_loss: 0.1930 | train_acc: 0.9234 | val_loss: 0.2247 | val_acc: 0.9062 | test_loss: 0.2160 | test_acc: 0.9117
Epoch: 20 | train_loss: 0.1925 | train_acc: 0.9231 | val_loss: 0.2232 | val_acc: 0.9074 | test_loss: 0.2128 | test_acc: 0.9132
Epoch: 21 | train_loss: 0.1911 | train_acc: 0.9231 | val_loss: 0.2225 | val_acc: 0.9070 | test_loss: 0.2125 | test_acc: 0.9138
Epoch: 22 | train_loss: 0.1907 | train_acc: 0.9232 | val_loss: 0.2218 | val_acc: 0.9079 | test_loss: 0.2157 | test_acc: 0.9109
Epoch: 23 | train_loss: 0.1905 | train_acc: 0.9239 | val_loss: 0.2211 | val_acc: 0.9077 | test_loss: 0.2150 | test_acc: 0.9121
Epoch: 24 | train_loss: 0.1891 | train_acc: 0.9241 | val_loss: 0.2199 | val_acc: 0.9077 | test_loss: 0.2135 | test_acc: 0.9134
Epoch: 25 | train_loss: 0.1892 | train_acc: 0.9235 | val_loss: 0.2240 | val_acc: 0.9060 | test_loss: 0.2119 | test_acc: 0.9132
Epoch: 26 | train_loss: 0.1886 | train_acc: 0.9244 | val_loss: 0.2223 | val_acc: 0.9075 | test_loss: 0.2142 | test_acc: 0.9119
Epoch: 27 | train_loss: 0.1879 | train_acc: 0.9247 | val_loss: 0.2258 | val_acc: 0.9042 | test_loss: 0.2087 | test_acc: 0.9168
Epoch: 28 | train_loss: 0.1870 | train_acc: 0.9246 | val_loss: 0.2232 | val_acc: 0.9057 | test_loss: 0.2095 | test_acc: 0.9156
Epoch: 29 | train_loss: 0.1864 | train_acc: 0.9245 | val_loss: 0.2254 | val_acc: 0.9040 | test_loss: 0.2085 | test_acc: 0.9162
Epoch: 30 | train_loss: 0.1870 | train_acc: 0.9245 | val_loss: 0.2269 | val_acc: 0.9036 | test_loss: 0.2088 | test_acc: 0.9153
Epoch: 31 | train_loss: 0.1865 | train_acc: 0.9251 | val_loss: 0.2207 | val_acc: 0.9060 | test_loss: 0.2109 | test_acc: 0.9138
Epoch: 32 | train_loss: 0.1865 | train_acc: 0.9255 | val_loss: 0.2232 | val_acc: 0.9051 | test_loss: 0.2077 | test_acc: 0.9166
Epoch: 33 | train_loss: 0.1855 | train_acc: 0.9254 | val_loss: 0.2214 | val_acc: 0.9070 | test_loss: 0.2089 | test_acc: 0.9145
Epoch: 34 | train_loss: 0.1848 | train_acc: 0.9260 | val_loss: 0.2226 | val_acc: 0.9057 | test_loss: 0.2087 | test_acc: 0.9160
Epoch: 35 | train_loss: 0.1852 | train_acc: 0.9255 | val_loss: 0.2224 | val_acc: 0.9053 | test_loss: 0.2085 | test_acc: 0.9156
Epoch: 36 | train_loss: 0.1845 | train_acc: 0.9260 | val_loss: 0.2194 | val_acc: 0.9085 | test_loss: 0.2111 | test_acc: 0.9126
Epoch: 37 | train_loss: 0.1840 | train_acc: 0.9265 | val_loss: 0.2209 | val_acc: 0.9072 | test_loss: 0.2074 | test_acc: 0.9160
Epoch: 38 | train_loss: 0.1846 | train_acc: 0.9257 | val_loss: 0.2240 | val_acc: 0.9049 | test_loss: 0.2077 | test_acc: 0.9168
Epoch: 39 | train_loss: 0.1832 | train_acc: 0.9265 | val_loss: 0.2222 | val_acc: 0.9057 | test_loss: 0.2076 | test_acc: 0.9160
Epoch: 40 | train_loss: 0.1834 | train_acc: 0.9268 | val_loss: 0.2185 | val_acc: 0.9091 | test_loss: 0.2091 | test_acc: 0.9153
Epoch: 41 | train_loss: 0.1825 | train_acc: 0.9261 | val_loss: 0.2203 | val_acc: 0.9072 | test_loss: 0.2079 | test_acc: 0.9162
Epoch: 42 | train_loss: 0.1821 | train_acc: 0.9266 | val_loss: 0.2265 | val_acc: 0.9040 | test_loss: 0.2058 | test_acc: 0.9173
Epoch: 43 | train_loss: 0.1824 | train_acc: 0.9258 | val_loss: 0.2199 | val_acc: 0.9079 | test_loss: 0.2088 | test_acc: 0.9153
Epoch: 44 | train_loss: 0.1821 | train_acc: 0.9266 | val_loss: 0.2215 | val_acc: 0.9072 | test_loss: 0.2090 | test_acc: 0.9147
Epoch: 45 | train_loss: 0.1822 | train_acc: 0.9267 | val_loss: 0.2205 | val_acc: 0.9074 | test_loss: 0.2067 | test_acc: 0.9158
Epoch: 46 | train_loss: 0.1820 | train_acc: 0.9269 | val_loss: 0.2228 | val_acc: 0.9066 | test_loss: 0.2057 | test_acc: 0.9160
Epoch: 47 | train_loss: 0.1814 | train_acc: 0.9269 | val_loss: 0.2232 | val_acc: 0.9064 | test_loss: 0.2046 | test_acc: 0.9177
Epoch: 48 | train_loss: 0.1811 | train_acc: 0.9270 | val_loss: 0.2195 | val_acc: 0.9079 | test_loss: 0.2085 | test_acc: 0.9160
Epoch: 49 | train_loss: 0.1812 | train_acc: 0.9270 | val_loss: 0.2204 | val_acc: 0.9087 | test_loss: 0.2057 | test_acc: 0.9162
Epoch: 50 | train_loss: 0.1813 | train_acc: 0.9275 | val_loss: 0.2214 | val_acc: 0.9085 | test_loss: 0.2045 | test_acc: 0.9172
[INFO] Saving model to: /var/scratch/ase347/DeepSummit/checkpoints/saint_epochs_50_lr_1e-6_depth_2_dropout_5e-1.pth
