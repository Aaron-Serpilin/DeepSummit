torch version: 2.0.1
torchvision version: 0.15.2
mlxtend version: 0.23.4
numpy version: 2.0.1
Device is: cpu

[INFO] Data splits already exist under data/himalayas_data. Skipping split.
Tabular train dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f747c69fa30>
Tabular val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f747bd51000>
Tabular test dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f747bd510c0>

[INFO] Created SummaryWriter, saving to: runs/saint_runs/epochs_50_lr_1e-5_depth_1_dropout_5e-1_heads_8/2025-06-11--18:25:17
Epoch: 1 | train_loss: 0.2925 | train_acc: 0.8792 | val_loss: 0.2438 | val_acc: 0.8983 | test_loss: 0.2335 | test_acc: 0.9043
Epoch: 2 | train_loss: 0.2070 | train_acc: 0.9172 | val_loss: 0.2258 | val_acc: 0.9087 | test_loss: 0.2228 | test_acc: 0.9091
Epoch: 3 | train_loss: 0.1957 | train_acc: 0.9212 | val_loss: 0.2136 | val_acc: 0.9109 | test_loss: 0.2081 | test_acc: 0.9170
Epoch: 4 | train_loss: 0.1913 | train_acc: 0.9228 | val_loss: 0.2128 | val_acc: 0.9141 | test_loss: 0.2083 | test_acc: 0.9145
Epoch: 5 | train_loss: 0.1879 | train_acc: 0.9240 | val_loss: 0.2092 | val_acc: 0.9138 | test_loss: 0.2015 | test_acc: 0.9181
Epoch: 6 | train_loss: 0.1855 | train_acc: 0.9259 | val_loss: 0.2098 | val_acc: 0.9128 | test_loss: 0.2034 | test_acc: 0.9157
Epoch: 7 | train_loss: 0.1836 | train_acc: 0.9262 | val_loss: 0.2104 | val_acc: 0.9123 | test_loss: 0.2039 | test_acc: 0.9145
Epoch: 8 | train_loss: 0.1821 | train_acc: 0.9273 | val_loss: 0.2100 | val_acc: 0.9130 | test_loss: 0.2034 | test_acc: 0.9168
Epoch: 9 | train_loss: 0.1806 | train_acc: 0.9276 | val_loss: 0.2085 | val_acc: 0.9138 | test_loss: 0.2023 | test_acc: 0.9162
Epoch: 10 | train_loss: 0.1793 | train_acc: 0.9278 | val_loss: 0.2139 | val_acc: 0.9077 | test_loss: 0.1997 | test_acc: 0.9170
Epoch: 11 | train_loss: 0.1786 | train_acc: 0.9277 | val_loss: 0.2112 | val_acc: 0.9126 | test_loss: 0.2065 | test_acc: 0.9140
Epoch: 12 | train_loss: 0.1777 | train_acc: 0.9283 | val_loss: 0.2148 | val_acc: 0.9090 | test_loss: 0.1997 | test_acc: 0.9162
Epoch: 13 | train_loss: 0.1762 | train_acc: 0.9287 | val_loss: 0.2191 | val_acc: 0.9057 | test_loss: 0.2001 | test_acc: 0.9170
Epoch: 14 | train_loss: 0.1761 | train_acc: 0.9295 | val_loss: 0.2154 | val_acc: 0.9085 | test_loss: 0.1997 | test_acc: 0.9166
Epoch: 15 | train_loss: 0.1755 | train_acc: 0.9293 | val_loss: 0.2198 | val_acc: 0.9074 | test_loss: 0.2016 | test_acc: 0.9172
Epoch: 16 | train_loss: 0.1748 | train_acc: 0.9302 | val_loss: 0.2199 | val_acc: 0.9100 | test_loss: 0.2010 | test_acc: 0.9179
Epoch: 17 | train_loss: 0.1745 | train_acc: 0.9295 | val_loss: 0.2167 | val_acc: 0.9092 | test_loss: 0.2017 | test_acc: 0.9157
Epoch: 18 | train_loss: 0.1732 | train_acc: 0.9308 | val_loss: 0.2286 | val_acc: 0.9044 | test_loss: 0.2003 | test_acc: 0.9175
Epoch: 19 | train_loss: 0.1735 | train_acc: 0.9299 | val_loss: 0.2200 | val_acc: 0.9087 | test_loss: 0.2021 | test_acc: 0.9157
Epoch: 20 | train_loss: 0.1730 | train_acc: 0.9298 | val_loss: 0.2274 | val_acc: 0.9055 | test_loss: 0.2014 | test_acc: 0.9168
Epoch: 21 | train_loss: 0.1720 | train_acc: 0.9307 | val_loss: 0.2255 | val_acc: 0.9075 | test_loss: 0.2002 | test_acc: 0.9172
Epoch: 22 | train_loss: 0.1713 | train_acc: 0.9311 | val_loss: 0.2245 | val_acc: 0.9106 | test_loss: 0.2079 | test_acc: 0.9147
Epoch: 23 | train_loss: 0.1710 | train_acc: 0.9314 | val_loss: 0.2285 | val_acc: 0.9066 | test_loss: 0.2027 | test_acc: 0.9166
Epoch: 24 | train_loss: 0.1706 | train_acc: 0.9313 | val_loss: 0.2236 | val_acc: 0.9089 | test_loss: 0.2022 | test_acc: 0.9155
Epoch: 25 | train_loss: 0.1697 | train_acc: 0.9317 | val_loss: 0.2262 | val_acc: 0.9081 | test_loss: 0.2029 | test_acc: 0.9141
Epoch: 26 | train_loss: 0.1694 | train_acc: 0.9318 | val_loss: 0.2280 | val_acc: 0.9083 | test_loss: 0.2028 | test_acc: 0.9147
Epoch: 27 | train_loss: 0.1689 | train_acc: 0.9324 | val_loss: 0.2306 | val_acc: 0.9074 | test_loss: 0.2041 | test_acc: 0.9145
Epoch: 28 | train_loss: 0.1685 | train_acc: 0.9318 | val_loss: 0.2340 | val_acc: 0.9064 | test_loss: 0.2056 | test_acc: 0.9140
Epoch: 29 | train_loss: 0.1678 | train_acc: 0.9324 | val_loss: 0.2415 | val_acc: 0.9083 | test_loss: 0.2049 | test_acc: 0.9162
Epoch: 30 | train_loss: 0.1669 | train_acc: 0.9328 | val_loss: 0.2463 | val_acc: 0.9058 | test_loss: 0.2062 | test_acc: 0.9168
Epoch: 31 | train_loss: 0.1669 | train_acc: 0.9323 | val_loss: 0.2350 | val_acc: 0.9062 | test_loss: 0.2039 | test_acc: 0.9145
Epoch: 32 | train_loss: 0.1662 | train_acc: 0.9331 | val_loss: 0.2285 | val_acc: 0.9085 | test_loss: 0.2036 | test_acc: 0.9141
Epoch: 33 | train_loss: 0.1663 | train_acc: 0.9326 | val_loss: 0.2490 | val_acc: 0.9034 | test_loss: 0.2047 | test_acc: 0.9153
Epoch: 34 | train_loss: 0.1658 | train_acc: 0.9331 | val_loss: 0.2426 | val_acc: 0.9077 | test_loss: 0.2081 | test_acc: 0.9160
Epoch: 35 | train_loss: 0.1653 | train_acc: 0.9331 | val_loss: 0.2272 | val_acc: 0.9096 | test_loss: 0.2048 | test_acc: 0.9151
Epoch: 36 | train_loss: 0.1639 | train_acc: 0.9335 | val_loss: 0.2365 | val_acc: 0.9075 | test_loss: 0.2079 | test_acc: 0.9134
Epoch: 37 | train_loss: 0.1639 | train_acc: 0.9339 | val_loss: 0.2565 | val_acc: 0.9042 | test_loss: 0.2085 | test_acc: 0.9170
Epoch: 38 | train_loss: 0.1643 | train_acc: 0.9342 | val_loss: 0.2370 | val_acc: 0.9075 | test_loss: 0.2035 | test_acc: 0.9164
Epoch: 39 | train_loss: 0.1627 | train_acc: 0.9340 | val_loss: 0.2496 | val_acc: 0.9066 | test_loss: 0.2084 | test_acc: 0.9143
Epoch: 40 | train_loss: 0.1626 | train_acc: 0.9342 | val_loss: 0.2486 | val_acc: 0.9070 | test_loss: 0.2097 | test_acc: 0.9141
Epoch: 41 | train_loss: 0.1623 | train_acc: 0.9341 | val_loss: 0.2437 | val_acc: 0.9064 | test_loss: 0.2062 | test_acc: 0.9145
Epoch: 42 | train_loss: 0.1618 | train_acc: 0.9343 | val_loss: 0.2541 | val_acc: 0.9070 | test_loss: 0.2122 | test_acc: 0.9141
Epoch: 43 | train_loss: 0.1619 | train_acc: 0.9351 | val_loss: 0.2480 | val_acc: 0.9070 | test_loss: 0.2085 | test_acc: 0.9141
Epoch: 44 | train_loss: 0.1603 | train_acc: 0.9353 | val_loss: 0.2434 | val_acc: 0.9068 | test_loss: 0.2094 | test_acc: 0.9136
Epoch: 45 | train_loss: 0.1611 | train_acc: 0.9351 | val_loss: 0.2662 | val_acc: 0.9045 | test_loss: 0.2109 | test_acc: 0.9149
Epoch: 46 | train_loss: 0.1600 | train_acc: 0.9354 | val_loss: 0.2613 | val_acc: 0.9057 | test_loss: 0.2112 | test_acc: 0.9160
Epoch: 47 | train_loss: 0.1594 | train_acc: 0.9355 | val_loss: 0.2574 | val_acc: 0.9077 | test_loss: 0.2108 | test_acc: 0.9155
Epoch: 48 | train_loss: 0.1595 | train_acc: 0.9358 | val_loss: 0.2544 | val_acc: 0.9070 | test_loss: 0.2099 | test_acc: 0.9153
Epoch: 49 | train_loss: 0.1582 | train_acc: 0.9356 | val_loss: 0.2677 | val_acc: 0.9040 | test_loss: 0.2131 | test_acc: 0.9153
Epoch: 50 | train_loss: 0.1580 | train_acc: 0.9366 | val_loss: 0.2585 | val_acc: 0.9068 | test_loss: 0.2108 | test_acc: 0.9140
[INFO] Saving model to: /var/scratch/ase347/DeepSummit/checkpoints/saint_epochs_50_lr_1e-5_depth_1_dropout_5e-1_heads_8.pth
