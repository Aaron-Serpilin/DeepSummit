torch version: 2.0.1
torchvision version: 0.15.2
mlxtend version: 0.23.4
numpy version: 2.0.1
Device is: cpu

[INFO] Data splits already exist under data/himalayas_data. Skipping split.
Tabular train dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fc00b2bfa30>
Tabular val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fc00a971000>
Tabular test dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fc00a9710c0>

[INFO] Created SummaryWriter, saving to: runs/saint_runs/saint_model/2025-06-16--13:56:55
Epoch: 1 | train_loss: 0.3035 | train_acc: 0.8756 | val_loss: 0.2353 | val_acc: 0.9030 | test_loss: 0.2386 | test_acc: 0.8985
Epoch: 2 | train_loss: 0.2085 | train_acc: 0.9158 | val_loss: 0.2272 | val_acc: 0.9089 | test_loss: 0.2285 | test_acc: 0.9072
Epoch: 3 | train_loss: 0.1992 | train_acc: 0.9197 | val_loss: 0.2188 | val_acc: 0.9075 | test_loss: 0.2258 | test_acc: 0.9077
Epoch: 4 | train_loss: 0.1938 | train_acc: 0.9211 | val_loss: 0.2189 | val_acc: 0.9109 | test_loss: 0.2166 | test_acc: 0.9138
Epoch: 5 | train_loss: 0.1910 | train_acc: 0.9232 | val_loss: 0.2148 | val_acc: 0.9117 | test_loss: 0.2211 | test_acc: 0.9089
Epoch: 6 | train_loss: 0.1889 | train_acc: 0.9236 | val_loss: 0.2134 | val_acc: 0.9122 | test_loss: 0.2191 | test_acc: 0.9119
Epoch: 7 | train_loss: 0.1865 | train_acc: 0.9251 | val_loss: 0.2127 | val_acc: 0.9109 | test_loss: 0.2099 | test_acc: 0.9164
Epoch: 8 | train_loss: 0.1857 | train_acc: 0.9255 | val_loss: 0.2097 | val_acc: 0.9128 | test_loss: 0.2097 | test_acc: 0.9179
Epoch: 9 | train_loss: 0.1837 | train_acc: 0.9260 | val_loss: 0.2093 | val_acc: 0.9122 | test_loss: 0.2089 | test_acc: 0.9164
Epoch: 10 | train_loss: 0.1827 | train_acc: 0.9264 | val_loss: 0.2101 | val_acc: 0.9119 | test_loss: 0.2087 | test_acc: 0.9166
Epoch: 11 | train_loss: 0.1815 | train_acc: 0.9269 | val_loss: 0.2096 | val_acc: 0.9134 | test_loss: 0.2098 | test_acc: 0.9157
Epoch: 12 | train_loss: 0.1810 | train_acc: 0.9277 | val_loss: 0.2105 | val_acc: 0.9149 | test_loss: 0.2149 | test_acc: 0.9089
Epoch: 13 | train_loss: 0.1801 | train_acc: 0.9280 | val_loss: 0.2076 | val_acc: 0.9121 | test_loss: 0.2084 | test_acc: 0.9157
Epoch: 14 | train_loss: 0.1792 | train_acc: 0.9282 | val_loss: 0.2128 | val_acc: 0.9134 | test_loss: 0.2210 | test_acc: 0.9023
Epoch: 15 | train_loss: 0.1787 | train_acc: 0.9284 | val_loss: 0.2090 | val_acc: 0.9113 | test_loss: 0.2083 | test_acc: 0.9153
Epoch: 16 | train_loss: 0.1782 | train_acc: 0.9285 | val_loss: 0.2167 | val_acc: 0.9138 | test_loss: 0.2160 | test_acc: 0.9091
Epoch: 17 | train_loss: 0.1779 | train_acc: 0.9286 | val_loss: 0.2110 | val_acc: 0.9140 | test_loss: 0.2131 | test_acc: 0.9098
Epoch: 18 | train_loss: 0.1773 | train_acc: 0.9281 | val_loss: 0.2121 | val_acc: 0.9138 | test_loss: 0.2129 | test_acc: 0.9093
Epoch: 19 | train_loss: 0.1761 | train_acc: 0.9300 | val_loss: 0.2105 | val_acc: 0.9113 | test_loss: 0.2085 | test_acc: 0.9164
Epoch: 20 | train_loss: 0.1762 | train_acc: 0.9292 | val_loss: 0.2126 | val_acc: 0.9109 | test_loss: 0.2126 | test_acc: 0.9096
Epoch: 21 | train_loss: 0.1760 | train_acc: 0.9291 | val_loss: 0.2161 | val_acc: 0.9124 | test_loss: 0.2153 | test_acc: 0.9066
Epoch: 22 | train_loss: 0.1750 | train_acc: 0.9298 | val_loss: 0.2212 | val_acc: 0.9087 | test_loss: 0.2246 | test_acc: 0.9049
Epoch: 23 | train_loss: 0.1743 | train_acc: 0.9301 | val_loss: 0.2140 | val_acc: 0.9113 | test_loss: 0.2118 | test_acc: 0.9096
Epoch: 24 | train_loss: 0.1749 | train_acc: 0.9294 | val_loss: 0.2136 | val_acc: 0.9108 | test_loss: 0.2157 | test_acc: 0.9076
Epoch: 25 | train_loss: 0.1750 | train_acc: 0.9294 | val_loss: 0.2180 | val_acc: 0.9098 | test_loss: 0.2162 | test_acc: 0.9085
Epoch: 26 | train_loss: 0.1732 | train_acc: 0.9298 | val_loss: 0.2136 | val_acc: 0.9109 | test_loss: 0.2121 | test_acc: 0.9119
Epoch: 27 | train_loss: 0.1731 | train_acc: 0.9297 | val_loss: 0.2171 | val_acc: 0.9109 | test_loss: 0.2161 | test_acc: 0.9062
Epoch: 28 | train_loss: 0.1723 | train_acc: 0.9315 | val_loss: 0.2144 | val_acc: 0.9096 | test_loss: 0.2145 | test_acc: 0.9104
Epoch: 29 | train_loss: 0.1721 | train_acc: 0.9307 | val_loss: 0.2172 | val_acc: 0.9094 | test_loss: 0.2172 | test_acc: 0.9096
Epoch: 30 | train_loss: 0.1719 | train_acc: 0.9310 | val_loss: 0.2229 | val_acc: 0.9092 | test_loss: 0.2282 | test_acc: 0.9029
Epoch: 31 | train_loss: 0.1715 | train_acc: 0.9317 | val_loss: 0.2227 | val_acc: 0.9100 | test_loss: 0.2240 | test_acc: 0.9061
Epoch: 32 | train_loss: 0.1712 | train_acc: 0.9317 | val_loss: 0.2179 | val_acc: 0.9089 | test_loss: 0.2202 | test_acc: 0.9066
Epoch: 33 | train_loss: 0.1714 | train_acc: 0.9308 | val_loss: 0.2173 | val_acc: 0.9094 | test_loss: 0.2139 | test_acc: 0.9109
Epoch: 34 | train_loss: 0.1711 | train_acc: 0.9309 | val_loss: 0.2185 | val_acc: 0.9077 | test_loss: 0.2204 | test_acc: 0.9059
Epoch: 35 | train_loss: 0.1699 | train_acc: 0.9313 | val_loss: 0.2206 | val_acc: 0.9087 | test_loss: 0.2249 | test_acc: 0.9053
Epoch: 36 | train_loss: 0.1696 | train_acc: 0.9321 | val_loss: 0.2238 | val_acc: 0.9085 | test_loss: 0.2227 | test_acc: 0.9072
Epoch: 37 | train_loss: 0.1697 | train_acc: 0.9325 | val_loss: 0.2299 | val_acc: 0.9085 | test_loss: 0.2303 | test_acc: 0.9051
Epoch: 38 | train_loss: 0.1703 | train_acc: 0.9317 | val_loss: 0.2208 | val_acc: 0.9079 | test_loss: 0.2229 | test_acc: 0.9089
Epoch: 39 | train_loss: 0.1694 | train_acc: 0.9322 | val_loss: 0.2235 | val_acc: 0.9077 | test_loss: 0.2251 | test_acc: 0.9096
Epoch: 40 | train_loss: 0.1693 | train_acc: 0.9321 | val_loss: 0.2210 | val_acc: 0.9089 | test_loss: 0.2229 | test_acc: 0.9064
Epoch: 41 | train_loss: 0.1678 | train_acc: 0.9328 | val_loss: 0.2200 | val_acc: 0.9077 | test_loss: 0.2249 | test_acc: 0.9040
Epoch: 42 | train_loss: 0.1682 | train_acc: 0.9320 | val_loss: 0.2273 | val_acc: 0.9064 | test_loss: 0.2347 | test_acc: 0.9023
Epoch: 43 | train_loss: 0.1676 | train_acc: 0.9326 | val_loss: 0.2210 | val_acc: 0.9083 | test_loss: 0.2246 | test_acc: 0.9074
Epoch: 44 | train_loss: 0.1674 | train_acc: 0.9323 | val_loss: 0.2332 | val_acc: 0.9068 | test_loss: 0.2382 | test_acc: 0.9030
Epoch: 45 | train_loss: 0.1670 | train_acc: 0.9321 | val_loss: 0.2297 | val_acc: 0.9074 | test_loss: 0.2299 | test_acc: 0.9068
Epoch: 46 | train_loss: 0.1671 | train_acc: 0.9333 | val_loss: 0.2238 | val_acc: 0.9072 | test_loss: 0.2269 | test_acc: 0.9066
Epoch: 47 | train_loss: 0.1669 | train_acc: 0.9326 | val_loss: 0.2289 | val_acc: 0.9064 | test_loss: 0.2280 | test_acc: 0.9064
Epoch: 48 | train_loss: 0.1672 | train_acc: 0.9331 | val_loss: 0.2275 | val_acc: 0.9070 | test_loss: 0.2328 | test_acc: 0.9044
Epoch: 49 | train_loss: 0.1667 | train_acc: 0.9334 | val_loss: 0.2295 | val_acc: 0.9072 | test_loss: 0.2309 | test_acc: 0.9053
Epoch: 50 | train_loss: 0.1659 | train_acc: 0.9329 | val_loss: 0.2326 | val_acc: 0.9077 | test_loss: 0.2347 | test_acc: 0.9055
[INFO] Saving model to: /var/scratch/ase347/DeepSummit/checkpoints/saint_model.pth
