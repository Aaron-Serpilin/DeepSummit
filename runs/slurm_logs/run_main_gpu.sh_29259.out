torch version: 2.0.1
torchvision version: 0.15.2
mlxtend version: 0.23.4
numpy version: 2.0.1
Device is: cpu

[INFO] Data splits already exist under data/himalayas_data. Skipping split.
Tabular train dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f787dbb9450>
Tabular val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f787dbb9a20>
Tabular test dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f787dbb9ae0>

[INFO] Created SummaryWriter, saving to: runs/saint_runs/epochs_50_lr_1e-5_depth_2_dropout_5e-1_heads_4_dimhead_4_dim_16/2025-06-11--21:32:01
Epoch: 1 | train_loss: 0.3250 | train_acc: 0.8633 | val_loss: 0.2672 | val_acc: 0.8927 | test_loss: 0.2552 | test_acc: 0.9000
Epoch: 2 | train_loss: 0.2151 | train_acc: 0.9138 | val_loss: 0.2417 | val_acc: 0.8979 | test_loss: 0.2244 | test_acc: 0.9055
Epoch: 3 | train_loss: 0.2028 | train_acc: 0.9189 | val_loss: 0.2395 | val_acc: 0.9002 | test_loss: 0.2178 | test_acc: 0.9094
Epoch: 4 | train_loss: 0.1964 | train_acc: 0.9212 | val_loss: 0.2353 | val_acc: 0.9034 | test_loss: 0.2148 | test_acc: 0.9106
Epoch: 5 | train_loss: 0.1935 | train_acc: 0.9222 | val_loss: 0.2365 | val_acc: 0.9036 | test_loss: 0.2151 | test_acc: 0.9109
Epoch: 6 | train_loss: 0.1906 | train_acc: 0.9237 | val_loss: 0.2341 | val_acc: 0.9015 | test_loss: 0.2095 | test_acc: 0.9143
Epoch: 7 | train_loss: 0.1889 | train_acc: 0.9235 | val_loss: 0.2356 | val_acc: 0.8994 | test_loss: 0.2095 | test_acc: 0.9134
Epoch: 8 | train_loss: 0.1863 | train_acc: 0.9251 | val_loss: 0.2316 | val_acc: 0.9028 | test_loss: 0.2076 | test_acc: 0.9119
Epoch: 9 | train_loss: 0.1847 | train_acc: 0.9256 | val_loss: 0.2351 | val_acc: 0.9011 | test_loss: 0.2087 | test_acc: 0.9136
Epoch: 10 | train_loss: 0.1839 | train_acc: 0.9269 | val_loss: 0.2380 | val_acc: 0.9017 | test_loss: 0.2091 | test_acc: 0.9136
Epoch: 11 | train_loss: 0.1830 | train_acc: 0.9256 | val_loss: 0.2273 | val_acc: 0.9055 | test_loss: 0.2073 | test_acc: 0.9147
Epoch: 12 | train_loss: 0.1818 | train_acc: 0.9276 | val_loss: 0.2325 | val_acc: 0.9043 | test_loss: 0.2076 | test_acc: 0.9132
Epoch: 13 | train_loss: 0.1810 | train_acc: 0.9277 | val_loss: 0.2276 | val_acc: 0.9058 | test_loss: 0.2072 | test_acc: 0.9125
Epoch: 14 | train_loss: 0.1797 | train_acc: 0.9277 | val_loss: 0.2277 | val_acc: 0.9058 | test_loss: 0.2073 | test_acc: 0.9143
Epoch: 15 | train_loss: 0.1793 | train_acc: 0.9278 | val_loss: 0.2286 | val_acc: 0.9070 | test_loss: 0.2068 | test_acc: 0.9147
Epoch: 16 | train_loss: 0.1784 | train_acc: 0.9283 | val_loss: 0.2259 | val_acc: 0.9060 | test_loss: 0.2058 | test_acc: 0.9126
Epoch: 17 | train_loss: 0.1780 | train_acc: 0.9276 | val_loss: 0.2302 | val_acc: 0.9060 | test_loss: 0.2162 | test_acc: 0.9111
Epoch: 18 | train_loss: 0.1773 | train_acc: 0.9289 | val_loss: 0.2273 | val_acc: 0.9058 | test_loss: 0.2091 | test_acc: 0.9108
Epoch: 19 | train_loss: 0.1771 | train_acc: 0.9287 | val_loss: 0.2267 | val_acc: 0.9058 | test_loss: 0.2102 | test_acc: 0.9126
Epoch: 20 | train_loss: 0.1769 | train_acc: 0.9292 | val_loss: 0.2307 | val_acc: 0.9023 | test_loss: 0.2070 | test_acc: 0.9125
Epoch: 21 | train_loss: 0.1762 | train_acc: 0.9294 | val_loss: 0.2333 | val_acc: 0.9019 | test_loss: 0.2079 | test_acc: 0.9128
Epoch: 22 | train_loss: 0.1761 | train_acc: 0.9294 | val_loss: 0.2271 | val_acc: 0.9049 | test_loss: 0.2098 | test_acc: 0.9115
Epoch: 23 | train_loss: 0.1765 | train_acc: 0.9294 | val_loss: 0.2293 | val_acc: 0.9040 | test_loss: 0.2088 | test_acc: 0.9113
Epoch: 24 | train_loss: 0.1750 | train_acc: 0.9298 | val_loss: 0.2251 | val_acc: 0.9066 | test_loss: 0.2101 | test_acc: 0.9115
Epoch: 25 | train_loss: 0.1742 | train_acc: 0.9301 | val_loss: 0.2320 | val_acc: 0.9030 | test_loss: 0.2088 | test_acc: 0.9111
Epoch: 26 | train_loss: 0.1746 | train_acc: 0.9301 | val_loss: 0.2258 | val_acc: 0.9056 | test_loss: 0.2074 | test_acc: 0.9125
Epoch: 27 | train_loss: 0.1748 | train_acc: 0.9296 | val_loss: 0.2379 | val_acc: 0.9004 | test_loss: 0.2090 | test_acc: 0.9128
Epoch: 28 | train_loss: 0.1743 | train_acc: 0.9302 | val_loss: 0.2331 | val_acc: 0.9032 | test_loss: 0.2071 | test_acc: 0.9145
Epoch: 29 | train_loss: 0.1740 | train_acc: 0.9298 | val_loss: 0.2296 | val_acc: 0.9047 | test_loss: 0.2068 | test_acc: 0.9145
Epoch: 30 | train_loss: 0.1737 | train_acc: 0.9307 | val_loss: 0.2302 | val_acc: 0.9030 | test_loss: 0.2089 | test_acc: 0.9126
Epoch: 31 | train_loss: 0.1735 | train_acc: 0.9302 | val_loss: 0.2324 | val_acc: 0.9038 | test_loss: 0.2089 | test_acc: 0.9145
Epoch: 32 | train_loss: 0.1726 | train_acc: 0.9300 | val_loss: 0.2333 | val_acc: 0.9038 | test_loss: 0.2077 | test_acc: 0.9155
Epoch: 33 | train_loss: 0.1724 | train_acc: 0.9305 | val_loss: 0.2326 | val_acc: 0.9008 | test_loss: 0.2124 | test_acc: 0.9123
Epoch: 34 | train_loss: 0.1722 | train_acc: 0.9307 | val_loss: 0.2287 | val_acc: 0.9058 | test_loss: 0.2096 | test_acc: 0.9128
Epoch: 35 | train_loss: 0.1717 | train_acc: 0.9311 | val_loss: 0.2309 | val_acc: 0.9058 | test_loss: 0.2108 | test_acc: 0.9115
Epoch: 36 | train_loss: 0.1709 | train_acc: 0.9311 | val_loss: 0.2377 | val_acc: 0.9017 | test_loss: 0.2116 | test_acc: 0.9149
Epoch: 37 | train_loss: 0.1714 | train_acc: 0.9314 | val_loss: 0.2379 | val_acc: 0.9009 | test_loss: 0.2139 | test_acc: 0.9104
Epoch: 38 | train_loss: 0.1710 | train_acc: 0.9302 | val_loss: 0.2355 | val_acc: 0.9036 | test_loss: 0.2098 | test_acc: 0.9134
Epoch: 39 | train_loss: 0.1706 | train_acc: 0.9310 | val_loss: 0.2288 | val_acc: 0.9064 | test_loss: 0.2112 | test_acc: 0.9125
Epoch: 40 | train_loss: 0.1702 | train_acc: 0.9320 | val_loss: 0.2361 | val_acc: 0.9028 | test_loss: 0.2123 | test_acc: 0.9119
Epoch: 41 | train_loss: 0.1704 | train_acc: 0.9309 | val_loss: 0.2351 | val_acc: 0.9028 | test_loss: 0.2108 | test_acc: 0.9130
Epoch: 42 | train_loss: 0.1693 | train_acc: 0.9319 | val_loss: 0.2322 | val_acc: 0.9036 | test_loss: 0.2113 | test_acc: 0.9121
Epoch: 43 | train_loss: 0.1688 | train_acc: 0.9316 | val_loss: 0.2355 | val_acc: 0.9030 | test_loss: 0.2132 | test_acc: 0.9102
Epoch: 44 | train_loss: 0.1686 | train_acc: 0.9327 | val_loss: 0.2433 | val_acc: 0.9000 | test_loss: 0.2109 | test_acc: 0.9126
Epoch: 45 | train_loss: 0.1688 | train_acc: 0.9321 | val_loss: 0.2454 | val_acc: 0.9011 | test_loss: 0.2117 | test_acc: 0.9147
Epoch: 46 | train_loss: 0.1686 | train_acc: 0.9317 | val_loss: 0.2331 | val_acc: 0.9047 | test_loss: 0.2140 | test_acc: 0.9109
Epoch: 47 | train_loss: 0.1677 | train_acc: 0.9325 | val_loss: 0.2371 | val_acc: 0.9015 | test_loss: 0.2137 | test_acc: 0.9113
Epoch: 48 | train_loss: 0.1684 | train_acc: 0.9324 | val_loss: 0.2366 | val_acc: 0.9019 | test_loss: 0.2133 | test_acc: 0.9125
Epoch: 49 | train_loss: 0.1677 | train_acc: 0.9326 | val_loss: 0.2343 | val_acc: 0.9043 | test_loss: 0.2140 | test_acc: 0.9121
Epoch: 50 | train_loss: 0.1672 | train_acc: 0.9321 | val_loss: 0.2428 | val_acc: 0.8996 | test_loss: 0.2168 | test_acc: 0.9104
[INFO] Saving model to: /var/scratch/ase347/DeepSummit/checkpoints/saint_epochs_50_lr_1e-5_depth_2_dropout_5e-1_heads_4_dimhead_4_dim_16.pth
