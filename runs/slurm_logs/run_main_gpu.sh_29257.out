torch version: 2.0.1
torchvision version: 0.15.2
mlxtend version: 0.23.4
numpy version: 2.0.1
Device is: cpu

[INFO] Data splits already exist under data/himalayas_data. Skipping split.
Tabular train dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f8b553db9a0>
Tabular val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f8b54a88f70>
Tabular test dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f8b54a89030>

[INFO] Created SummaryWriter, saving to: runs/saint_runs/epochs_50_lr_1e-6_depth_2_dropout_75e-2_heads_8/2025-06-11--20:02:31
Epoch: 1 | train_loss: 0.5507 | train_acc: 0.7624 | val_loss: 0.4212 | val_acc: 0.8352 | test_loss: 0.4038 | test_acc: 0.8432
Epoch: 2 | train_loss: 0.3515 | train_acc: 0.8544 | val_loss: 0.3266 | val_acc: 0.8626 | test_loss: 0.3104 | test_acc: 0.8620
Epoch: 3 | train_loss: 0.2980 | train_acc: 0.8728 | val_loss: 0.2965 | val_acc: 0.8772 | test_loss: 0.2804 | test_acc: 0.8816
Epoch: 4 | train_loss: 0.2751 | train_acc: 0.8836 | val_loss: 0.2796 | val_acc: 0.8838 | test_loss: 0.2683 | test_acc: 0.8889
Epoch: 5 | train_loss: 0.2626 | train_acc: 0.8915 | val_loss: 0.2697 | val_acc: 0.8925 | test_loss: 0.2670 | test_acc: 0.8964
Epoch: 6 | train_loss: 0.2530 | train_acc: 0.8972 | val_loss: 0.2638 | val_acc: 0.8893 | test_loss: 0.2540 | test_acc: 0.9021
Epoch: 7 | train_loss: 0.2439 | train_acc: 0.9014 | val_loss: 0.2577 | val_acc: 0.8934 | test_loss: 0.2545 | test_acc: 0.9044
Epoch: 8 | train_loss: 0.2367 | train_acc: 0.9049 | val_loss: 0.2533 | val_acc: 0.8927 | test_loss: 0.2458 | test_acc: 0.9047
Epoch: 9 | train_loss: 0.2308 | train_acc: 0.9070 | val_loss: 0.2501 | val_acc: 0.8925 | test_loss: 0.2407 | test_acc: 0.9049
Epoch: 10 | train_loss: 0.2243 | train_acc: 0.9117 | val_loss: 0.2446 | val_acc: 0.8970 | test_loss: 0.2373 | test_acc: 0.9060
Epoch: 11 | train_loss: 0.2192 | train_acc: 0.9135 | val_loss: 0.2401 | val_acc: 0.9011 | test_loss: 0.2372 | test_acc: 0.9059
Epoch: 12 | train_loss: 0.2167 | train_acc: 0.9140 | val_loss: 0.2393 | val_acc: 0.8996 | test_loss: 0.2289 | test_acc: 0.9068
Epoch: 13 | train_loss: 0.2137 | train_acc: 0.9145 | val_loss: 0.2375 | val_acc: 0.9036 | test_loss: 0.2293 | test_acc: 0.9064
Epoch: 14 | train_loss: 0.2103 | train_acc: 0.9160 | val_loss: 0.2327 | val_acc: 0.9058 | test_loss: 0.2298 | test_acc: 0.9066
Epoch: 15 | train_loss: 0.2091 | train_acc: 0.9169 | val_loss: 0.2316 | val_acc: 0.9058 | test_loss: 0.2329 | test_acc: 0.9060
Epoch: 16 | train_loss: 0.2069 | train_acc: 0.9172 | val_loss: 0.2315 | val_acc: 0.9058 | test_loss: 0.2244 | test_acc: 0.9079
Epoch: 17 | train_loss: 0.2057 | train_acc: 0.9175 | val_loss: 0.2290 | val_acc: 0.9072 | test_loss: 0.2293 | test_acc: 0.9068
Epoch: 18 | train_loss: 0.2039 | train_acc: 0.9178 | val_loss: 0.2289 | val_acc: 0.9077 | test_loss: 0.2263 | test_acc: 0.9079
Epoch: 19 | train_loss: 0.2016 | train_acc: 0.9196 | val_loss: 0.2279 | val_acc: 0.9079 | test_loss: 0.2281 | test_acc: 0.9064
Epoch: 20 | train_loss: 0.2012 | train_acc: 0.9197 | val_loss: 0.2266 | val_acc: 0.9085 | test_loss: 0.2245 | test_acc: 0.9079
Epoch: 21 | train_loss: 0.2002 | train_acc: 0.9195 | val_loss: 0.2260 | val_acc: 0.9079 | test_loss: 0.2243 | test_acc: 0.9079
Epoch: 22 | train_loss: 0.1997 | train_acc: 0.9198 | val_loss: 0.2262 | val_acc: 0.9081 | test_loss: 0.2268 | test_acc: 0.9060
Epoch: 23 | train_loss: 0.1992 | train_acc: 0.9202 | val_loss: 0.2256 | val_acc: 0.9077 | test_loss: 0.2263 | test_acc: 0.9062
Epoch: 24 | train_loss: 0.1979 | train_acc: 0.9205 | val_loss: 0.2239 | val_acc: 0.9087 | test_loss: 0.2239 | test_acc: 0.9072
Epoch: 25 | train_loss: 0.1963 | train_acc: 0.9211 | val_loss: 0.2246 | val_acc: 0.9074 | test_loss: 0.2224 | test_acc: 0.9074
Epoch: 26 | train_loss: 0.1965 | train_acc: 0.9215 | val_loss: 0.2247 | val_acc: 0.9085 | test_loss: 0.2241 | test_acc: 0.9070
Epoch: 27 | train_loss: 0.1967 | train_acc: 0.9202 | val_loss: 0.2235 | val_acc: 0.9085 | test_loss: 0.2177 | test_acc: 0.9094
Epoch: 28 | train_loss: 0.1948 | train_acc: 0.9221 | val_loss: 0.2233 | val_acc: 0.9087 | test_loss: 0.2200 | test_acc: 0.9091
Epoch: 29 | train_loss: 0.1944 | train_acc: 0.9214 | val_loss: 0.2238 | val_acc: 0.9081 | test_loss: 0.2181 | test_acc: 0.9098
Epoch: 30 | train_loss: 0.1954 | train_acc: 0.9220 | val_loss: 0.2243 | val_acc: 0.9081 | test_loss: 0.2192 | test_acc: 0.9096
Epoch: 31 | train_loss: 0.1940 | train_acc: 0.9217 | val_loss: 0.2224 | val_acc: 0.9106 | test_loss: 0.2214 | test_acc: 0.9089
Epoch: 32 | train_loss: 0.1935 | train_acc: 0.9218 | val_loss: 0.2222 | val_acc: 0.9090 | test_loss: 0.2169 | test_acc: 0.9104
Epoch: 33 | train_loss: 0.1935 | train_acc: 0.9217 | val_loss: 0.2224 | val_acc: 0.9090 | test_loss: 0.2190 | test_acc: 0.9100
Epoch: 34 | train_loss: 0.1922 | train_acc: 0.9214 | val_loss: 0.2214 | val_acc: 0.9092 | test_loss: 0.2170 | test_acc: 0.9111
Epoch: 35 | train_loss: 0.1924 | train_acc: 0.9226 | val_loss: 0.2216 | val_acc: 0.9096 | test_loss: 0.2169 | test_acc: 0.9106
Epoch: 36 | train_loss: 0.1919 | train_acc: 0.9234 | val_loss: 0.2205 | val_acc: 0.9115 | test_loss: 0.2205 | test_acc: 0.9072
Epoch: 37 | train_loss: 0.1914 | train_acc: 0.9231 | val_loss: 0.2194 | val_acc: 0.9115 | test_loss: 0.2159 | test_acc: 0.9111
Epoch: 38 | train_loss: 0.1917 | train_acc: 0.9232 | val_loss: 0.2214 | val_acc: 0.9087 | test_loss: 0.2155 | test_acc: 0.9111
Epoch: 39 | train_loss: 0.1906 | train_acc: 0.9237 | val_loss: 0.2207 | val_acc: 0.9102 | test_loss: 0.2162 | test_acc: 0.9104
Epoch: 40 | train_loss: 0.1898 | train_acc: 0.9230 | val_loss: 0.2196 | val_acc: 0.9119 | test_loss: 0.2179 | test_acc: 0.9100
Epoch: 41 | train_loss: 0.1888 | train_acc: 0.9241 | val_loss: 0.2200 | val_acc: 0.9104 | test_loss: 0.2163 | test_acc: 0.9109
Epoch: 42 | train_loss: 0.1901 | train_acc: 0.9236 | val_loss: 0.2213 | val_acc: 0.9089 | test_loss: 0.2131 | test_acc: 0.9136
Epoch: 43 | train_loss: 0.1895 | train_acc: 0.9239 | val_loss: 0.2203 | val_acc: 0.9092 | test_loss: 0.2152 | test_acc: 0.9123
Epoch: 44 | train_loss: 0.1893 | train_acc: 0.9234 | val_loss: 0.2205 | val_acc: 0.9102 | test_loss: 0.2180 | test_acc: 0.9089
Epoch: 45 | train_loss: 0.1892 | train_acc: 0.9244 | val_loss: 0.2188 | val_acc: 0.9102 | test_loss: 0.2134 | test_acc: 0.9124
Epoch: 46 | train_loss: 0.1882 | train_acc: 0.9249 | val_loss: 0.2197 | val_acc: 0.9102 | test_loss: 0.2136 | test_acc: 0.9121
Epoch: 47 | train_loss: 0.1889 | train_acc: 0.9242 | val_loss: 0.2202 | val_acc: 0.9098 | test_loss: 0.2121 | test_acc: 0.9136
Epoch: 48 | train_loss: 0.1882 | train_acc: 0.9242 | val_loss: 0.2189 | val_acc: 0.9117 | test_loss: 0.2178 | test_acc: 0.9098
Epoch: 49 | train_loss: 0.1876 | train_acc: 0.9245 | val_loss: 0.2185 | val_acc: 0.9106 | test_loss: 0.2138 | test_acc: 0.9124
Epoch: 50 | train_loss: 0.1876 | train_acc: 0.9252 | val_loss: 0.2177 | val_acc: 0.9117 | test_loss: 0.2129 | test_acc: 0.9126
[INFO] Saving model to: /var/scratch/ase347/DeepSummit/checkpoints/saint_epochs_50_lr_1e-6_depth_2_dropout_75e-2_heads_8.pth
