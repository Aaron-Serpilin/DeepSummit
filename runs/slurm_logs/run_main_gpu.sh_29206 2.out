torch version: 2.0.1
torchvision version: 0.15.2
mlxtend version: 0.23.4
numpy version: 2.0.1
Device is: cpu

[INFO] Data splits already exist under data/himalayas_data. Skipping split.
Tabular train dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fa6a2d07a00>
Tabular val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fa6a23b8fd0>
Tabular test dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7fa6a23b9090>

[INFO] Created SummaryWriter, saving to: runs/saint_runs/epochs_50_lr_1e-6_depth_3_dropout_25e-2/2025-06-11--02:23:40
Epoch: 1 | train_loss: 0.4299 | train_acc: 0.8256 | val_loss: 0.3277 | val_acc: 0.8665 | test_loss: 0.2984 | test_acc: 0.8767
Epoch: 2 | train_loss: 0.2649 | train_acc: 0.8919 | val_loss: 0.2874 | val_acc: 0.8795 | test_loss: 0.2590 | test_acc: 0.9000
Epoch: 3 | train_loss: 0.2346 | train_acc: 0.9073 | val_loss: 0.2767 | val_acc: 0.8789 | test_loss: 0.2448 | test_acc: 0.9042
Epoch: 4 | train_loss: 0.2181 | train_acc: 0.9139 | val_loss: 0.2638 | val_acc: 0.8887 | test_loss: 0.2368 | test_acc: 0.9075
Epoch: 5 | train_loss: 0.2083 | train_acc: 0.9177 | val_loss: 0.2672 | val_acc: 0.8878 | test_loss: 0.2291 | test_acc: 0.9066
Epoch: 6 | train_loss: 0.2032 | train_acc: 0.9191 | val_loss: 0.2741 | val_acc: 0.8848 | test_loss: 0.2262 | test_acc: 0.9068
Epoch: 7 | train_loss: 0.1990 | train_acc: 0.9201 | val_loss: 0.2699 | val_acc: 0.8883 | test_loss: 0.2251 | test_acc: 0.9072
Epoch: 8 | train_loss: 0.1975 | train_acc: 0.9215 | val_loss: 0.2706 | val_acc: 0.8898 | test_loss: 0.2218 | test_acc: 0.9085
Epoch: 9 | train_loss: 0.1950 | train_acc: 0.9215 | val_loss: 0.2631 | val_acc: 0.8912 | test_loss: 0.2194 | test_acc: 0.9113
Epoch: 10 | train_loss: 0.1927 | train_acc: 0.9231 | val_loss: 0.2640 | val_acc: 0.8925 | test_loss: 0.2197 | test_acc: 0.9109
Epoch: 11 | train_loss: 0.1912 | train_acc: 0.9233 | val_loss: 0.2752 | val_acc: 0.8906 | test_loss: 0.2204 | test_acc: 0.9094
Epoch: 12 | train_loss: 0.1908 | train_acc: 0.9230 | val_loss: 0.2625 | val_acc: 0.8932 | test_loss: 0.2195 | test_acc: 0.9128
Epoch: 13 | train_loss: 0.1894 | train_acc: 0.9235 | val_loss: 0.2675 | val_acc: 0.8921 | test_loss: 0.2172 | test_acc: 0.9111
Epoch: 14 | train_loss: 0.1888 | train_acc: 0.9237 | val_loss: 0.2605 | val_acc: 0.8938 | test_loss: 0.2146 | test_acc: 0.9143
Epoch: 15 | train_loss: 0.1878 | train_acc: 0.9249 | val_loss: 0.2766 | val_acc: 0.8904 | test_loss: 0.2177 | test_acc: 0.9113
Epoch: 16 | train_loss: 0.1866 | train_acc: 0.9252 | val_loss: 0.2716 | val_acc: 0.8925 | test_loss: 0.2152 | test_acc: 0.9128
Epoch: 17 | train_loss: 0.1860 | train_acc: 0.9254 | val_loss: 0.2773 | val_acc: 0.8908 | test_loss: 0.2146 | test_acc: 0.9141
Epoch: 18 | train_loss: 0.1856 | train_acc: 0.9247 | val_loss: 0.2679 | val_acc: 0.8936 | test_loss: 0.2132 | test_acc: 0.9140
Epoch: 19 | train_loss: 0.1850 | train_acc: 0.9256 | val_loss: 0.2719 | val_acc: 0.8930 | test_loss: 0.2134 | test_acc: 0.9147
Epoch: 20 | train_loss: 0.1840 | train_acc: 0.9261 | val_loss: 0.2667 | val_acc: 0.8946 | test_loss: 0.2137 | test_acc: 0.9151
Epoch: 21 | train_loss: 0.1836 | train_acc: 0.9259 | val_loss: 0.2718 | val_acc: 0.8929 | test_loss: 0.2114 | test_acc: 0.9168
Epoch: 22 | train_loss: 0.1832 | train_acc: 0.9260 | val_loss: 0.2714 | val_acc: 0.8917 | test_loss: 0.2126 | test_acc: 0.9157
Epoch: 23 | train_loss: 0.1835 | train_acc: 0.9254 | val_loss: 0.2762 | val_acc: 0.8923 | test_loss: 0.2134 | test_acc: 0.9143
Epoch: 24 | train_loss: 0.1822 | train_acc: 0.9271 | val_loss: 0.2744 | val_acc: 0.8902 | test_loss: 0.2125 | test_acc: 0.9155
Epoch: 25 | train_loss: 0.1817 | train_acc: 0.9267 | val_loss: 0.2754 | val_acc: 0.8919 | test_loss: 0.2131 | test_acc: 0.9158
Epoch: 26 | train_loss: 0.1824 | train_acc: 0.9268 | val_loss: 0.2758 | val_acc: 0.8930 | test_loss: 0.2139 | test_acc: 0.9160
Epoch: 27 | train_loss: 0.1810 | train_acc: 0.9271 | val_loss: 0.2772 | val_acc: 0.8940 | test_loss: 0.2137 | test_acc: 0.9153
Epoch: 28 | train_loss: 0.1814 | train_acc: 0.9256 | val_loss: 0.2749 | val_acc: 0.8932 | test_loss: 0.2126 | test_acc: 0.9162
Epoch: 29 | train_loss: 0.1804 | train_acc: 0.9269 | val_loss: 0.2649 | val_acc: 0.8944 | test_loss: 0.2103 | test_acc: 0.9181
Epoch: 30 | train_loss: 0.1799 | train_acc: 0.9277 | val_loss: 0.2848 | val_acc: 0.8880 | test_loss: 0.2147 | test_acc: 0.9141
Epoch: 31 | train_loss: 0.1800 | train_acc: 0.9270 | val_loss: 0.2726 | val_acc: 0.8921 | test_loss: 0.2113 | test_acc: 0.9158
Epoch: 32 | train_loss: 0.1796 | train_acc: 0.9285 | val_loss: 0.2714 | val_acc: 0.8934 | test_loss: 0.2116 | test_acc: 0.9175
Epoch: 33 | train_loss: 0.1790 | train_acc: 0.9278 | val_loss: 0.2689 | val_acc: 0.8942 | test_loss: 0.2107 | test_acc: 0.9177
Epoch: 34 | train_loss: 0.1789 | train_acc: 0.9280 | val_loss: 0.2748 | val_acc: 0.8919 | test_loss: 0.2130 | test_acc: 0.9170
Epoch: 35 | train_loss: 0.1790 | train_acc: 0.9277 | val_loss: 0.2741 | val_acc: 0.8917 | test_loss: 0.2108 | test_acc: 0.9177
Epoch: 36 | train_loss: 0.1786 | train_acc: 0.9284 | val_loss: 0.2642 | val_acc: 0.8953 | test_loss: 0.2110 | test_acc: 0.9185
Epoch: 37 | train_loss: 0.1781 | train_acc: 0.9285 | val_loss: 0.2666 | val_acc: 0.8944 | test_loss: 0.2110 | test_acc: 0.9183
Epoch: 38 | train_loss: 0.1775 | train_acc: 0.9289 | val_loss: 0.2590 | val_acc: 0.8947 | test_loss: 0.2093 | test_acc: 0.9181
Epoch: 39 | train_loss: 0.1771 | train_acc: 0.9285 | val_loss: 0.2691 | val_acc: 0.8921 | test_loss: 0.2112 | test_acc: 0.9181
Epoch: 40 | train_loss: 0.1770 | train_acc: 0.9291 | val_loss: 0.2786 | val_acc: 0.8923 | test_loss: 0.2142 | test_acc: 0.9172
Epoch: 41 | train_loss: 0.1767 | train_acc: 0.9290 | val_loss: 0.2540 | val_acc: 0.8974 | test_loss: 0.2090 | test_acc: 0.9192
Epoch: 42 | train_loss: 0.1768 | train_acc: 0.9297 | val_loss: 0.2603 | val_acc: 0.8936 | test_loss: 0.2098 | test_acc: 0.9187
Epoch: 43 | train_loss: 0.1759 | train_acc: 0.9292 | val_loss: 0.2748 | val_acc: 0.8914 | test_loss: 0.2139 | test_acc: 0.9164
Epoch: 44 | train_loss: 0.1759 | train_acc: 0.9290 | val_loss: 0.2611 | val_acc: 0.8953 | test_loss: 0.2116 | test_acc: 0.9187
Epoch: 45 | train_loss: 0.1758 | train_acc: 0.9290 | val_loss: 0.2549 | val_acc: 0.8976 | test_loss: 0.2101 | test_acc: 0.9190
Epoch: 46 | train_loss: 0.1752 | train_acc: 0.9295 | val_loss: 0.2606 | val_acc: 0.8947 | test_loss: 0.2099 | test_acc: 0.9187
Epoch: 47 | train_loss: 0.1756 | train_acc: 0.9298 | val_loss: 0.2603 | val_acc: 0.8953 | test_loss: 0.2104 | test_acc: 0.9198
Epoch: 48 | train_loss: 0.1753 | train_acc: 0.9296 | val_loss: 0.2575 | val_acc: 0.8972 | test_loss: 0.2099 | test_acc: 0.9190
Epoch: 49 | train_loss: 0.1749 | train_acc: 0.9294 | val_loss: 0.2692 | val_acc: 0.8900 | test_loss: 0.2129 | test_acc: 0.9181
Epoch: 50 | train_loss: 0.1749 | train_acc: 0.9303 | val_loss: 0.2877 | val_acc: 0.8885 | test_loss: 0.2183 | test_acc: 0.9162
[INFO] Saving model to: /var/scratch/ase347/DeepSummit/checkpoints/saint_epochs_50_lr_1e-6_depth_3_dropout_25e-2.pth
