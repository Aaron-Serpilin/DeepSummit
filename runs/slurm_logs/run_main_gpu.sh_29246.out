torch version: 2.0.1
torchvision version: 0.15.2
mlxtend version: 0.23.4
numpy version: 2.0.1
Device is: cpu

[INFO] Data splits already exist under data/himalayas_data. Skipping split.
Tabular train dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f7edccdba90>
Tabular val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f7edc38d060>
Tabular test dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f7edc38d120>

[INFO] Created SummaryWriter, saving to: runs/saint_runs/epochs_50_lr_1e-5_depth_3_dropout_5e-1_decay_1e-1/2025-06-11--16:05:04
Epoch: 1 | train_loss: 0.2552 | train_acc: 0.8971 | val_loss: 0.2574 | val_acc: 0.8929 | test_loss: 0.2261 | test_acc: 0.9094
Epoch: 2 | train_loss: 0.1994 | train_acc: 0.9201 | val_loss: 0.2534 | val_acc: 0.8953 | test_loss: 0.2146 | test_acc: 0.9123
Epoch: 3 | train_loss: 0.1918 | train_acc: 0.9232 | val_loss: 0.2710 | val_acc: 0.8932 | test_loss: 0.2187 | test_acc: 0.9092
Epoch: 4 | train_loss: 0.1881 | train_acc: 0.9244 | val_loss: 0.2301 | val_acc: 0.9058 | test_loss: 0.2238 | test_acc: 0.9104
Epoch: 5 | train_loss: 0.1853 | train_acc: 0.9251 | val_loss: 0.2268 | val_acc: 0.9068 | test_loss: 0.2087 | test_acc: 0.9181
Epoch: 6 | train_loss: 0.1824 | train_acc: 0.9268 | val_loss: 0.2587 | val_acc: 0.8976 | test_loss: 0.2175 | test_acc: 0.9134
Epoch: 7 | train_loss: 0.1813 | train_acc: 0.9271 | val_loss: 0.2334 | val_acc: 0.9055 | test_loss: 0.2114 | test_acc: 0.9155
Epoch: 8 | train_loss: 0.1799 | train_acc: 0.9280 | val_loss: 0.2411 | val_acc: 0.9013 | test_loss: 0.2134 | test_acc: 0.9160
Epoch: 9 | train_loss: 0.1782 | train_acc: 0.9282 | val_loss: 0.2338 | val_acc: 0.9060 | test_loss: 0.2082 | test_acc: 0.9173
Epoch: 10 | train_loss: 0.1773 | train_acc: 0.9288 | val_loss: 0.2272 | val_acc: 0.9077 | test_loss: 0.2078 | test_acc: 0.9175
Epoch: 11 | train_loss: 0.1758 | train_acc: 0.9295 | val_loss: 0.2458 | val_acc: 0.9006 | test_loss: 0.2207 | test_acc: 0.9160
Epoch: 12 | train_loss: 0.1755 | train_acc: 0.9290 | val_loss: 0.2238 | val_acc: 0.9104 | test_loss: 0.2091 | test_acc: 0.9189
Epoch: 13 | train_loss: 0.1743 | train_acc: 0.9302 | val_loss: 0.2404 | val_acc: 0.9070 | test_loss: 0.2213 | test_acc: 0.9162
Epoch: 14 | train_loss: 0.1743 | train_acc: 0.9306 | val_loss: 0.2228 | val_acc: 0.9090 | test_loss: 0.2075 | test_acc: 0.9168
Epoch: 15 | train_loss: 0.1731 | train_acc: 0.9305 | val_loss: 0.2602 | val_acc: 0.8974 | test_loss: 0.2272 | test_acc: 0.9134
Epoch: 16 | train_loss: 0.1727 | train_acc: 0.9305 | val_loss: 0.2485 | val_acc: 0.9013 | test_loss: 0.2221 | test_acc: 0.9155
Epoch: 17 | train_loss: 0.1723 | train_acc: 0.9311 | val_loss: 0.2449 | val_acc: 0.9045 | test_loss: 0.2214 | test_acc: 0.9138
Epoch: 18 | train_loss: 0.1715 | train_acc: 0.9307 | val_loss: 0.2264 | val_acc: 0.9104 | test_loss: 0.2087 | test_acc: 0.9179
Epoch: 19 | train_loss: 0.1702 | train_acc: 0.9317 | val_loss: 0.2276 | val_acc: 0.9096 | test_loss: 0.2099 | test_acc: 0.9162
Epoch: 20 | train_loss: 0.1698 | train_acc: 0.9316 | val_loss: 0.2364 | val_acc: 0.9087 | test_loss: 0.2142 | test_acc: 0.9177
Epoch: 21 | train_loss: 0.1689 | train_acc: 0.9322 | val_loss: 0.2336 | val_acc: 0.9072 | test_loss: 0.2142 | test_acc: 0.9141
Epoch: 22 | train_loss: 0.1689 | train_acc: 0.9318 | val_loss: 0.2313 | val_acc: 0.9060 | test_loss: 0.2119 | test_acc: 0.9141
Epoch: 23 | train_loss: 0.1684 | train_acc: 0.9320 | val_loss: 0.2446 | val_acc: 0.9049 | test_loss: 0.2191 | test_acc: 0.9128
Epoch: 24 | train_loss: 0.1676 | train_acc: 0.9321 | val_loss: 0.2352 | val_acc: 0.9036 | test_loss: 0.2146 | test_acc: 0.9145
Epoch: 25 | train_loss: 0.1664 | train_acc: 0.9332 | val_loss: 0.2455 | val_acc: 0.9070 | test_loss: 0.2182 | test_acc: 0.9175
Epoch: 26 | train_loss: 0.1661 | train_acc: 0.9338 | val_loss: 0.2355 | val_acc: 0.9092 | test_loss: 0.2173 | test_acc: 0.9140
Epoch: 27 | train_loss: 0.1651 | train_acc: 0.9333 | val_loss: 0.2422 | val_acc: 0.9090 | test_loss: 0.2212 | test_acc: 0.9160
Epoch: 28 | train_loss: 0.1652 | train_acc: 0.9339 | val_loss: 0.2371 | val_acc: 0.9079 | test_loss: 0.2143 | test_acc: 0.9164
Epoch: 29 | train_loss: 0.1641 | train_acc: 0.9330 | val_loss: 0.2405 | val_acc: 0.9083 | test_loss: 0.2171 | test_acc: 0.9177
Epoch: 30 | train_loss: 0.1634 | train_acc: 0.9337 | val_loss: 0.2650 | val_acc: 0.9004 | test_loss: 0.2328 | test_acc: 0.9128
Epoch: 31 | train_loss: 0.1631 | train_acc: 0.9344 | val_loss: 0.2363 | val_acc: 0.9049 | test_loss: 0.2167 | test_acc: 0.9121
Epoch: 32 | train_loss: 0.1617 | train_acc: 0.9351 | val_loss: 0.2420 | val_acc: 0.9041 | test_loss: 0.2263 | test_acc: 0.9091
Epoch: 33 | train_loss: 0.1612 | train_acc: 0.9354 | val_loss: 0.2488 | val_acc: 0.9056 | test_loss: 0.2262 | test_acc: 0.9151
Epoch: 34 | train_loss: 0.1610 | train_acc: 0.9352 | val_loss: 0.2469 | val_acc: 0.9045 | test_loss: 0.2265 | test_acc: 0.9138
Epoch: 35 | train_loss: 0.1606 | train_acc: 0.9357 | val_loss: 0.2482 | val_acc: 0.9030 | test_loss: 0.2290 | test_acc: 0.9111
Epoch: 36 | train_loss: 0.1601 | train_acc: 0.9362 | val_loss: 0.2394 | val_acc: 0.9092 | test_loss: 0.2188 | test_acc: 0.9170
Epoch: 37 | train_loss: 0.1590 | train_acc: 0.9362 | val_loss: 0.2410 | val_acc: 0.9051 | test_loss: 0.2245 | test_acc: 0.9117
Epoch: 38 | train_loss: 0.1581 | train_acc: 0.9360 | val_loss: 0.2389 | val_acc: 0.9083 | test_loss: 0.2195 | test_acc: 0.9149
Epoch: 39 | train_loss: 0.1581 | train_acc: 0.9363 | val_loss: 0.2413 | val_acc: 0.9049 | test_loss: 0.2269 | test_acc: 0.9093
Epoch: 40 | train_loss: 0.1565 | train_acc: 0.9367 | val_loss: 0.2407 | val_acc: 0.9075 | test_loss: 0.2210 | test_acc: 0.9149
Epoch: 41 | train_loss: 0.1559 | train_acc: 0.9368 | val_loss: 0.2362 | val_acc: 0.9104 | test_loss: 0.2245 | test_acc: 0.9153
Epoch: 42 | train_loss: 0.1551 | train_acc: 0.9368 | val_loss: 0.2453 | val_acc: 0.9047 | test_loss: 0.2265 | test_acc: 0.9126
Epoch: 43 | train_loss: 0.1544 | train_acc: 0.9379 | val_loss: 0.2583 | val_acc: 0.9034 | test_loss: 0.2335 | test_acc: 0.9125
Epoch: 44 | train_loss: 0.1535 | train_acc: 0.9378 | val_loss: 0.2533 | val_acc: 0.9088 | test_loss: 0.2301 | test_acc: 0.9134
Epoch: 45 | train_loss: 0.1536 | train_acc: 0.9385 | val_loss: 0.2490 | val_acc: 0.9075 | test_loss: 0.2288 | test_acc: 0.9143
Epoch: 46 | train_loss: 0.1525 | train_acc: 0.9381 | val_loss: 0.2501 | val_acc: 0.9088 | test_loss: 0.2335 | test_acc: 0.9162
Epoch: 47 | train_loss: 0.1518 | train_acc: 0.9391 | val_loss: 0.2529 | val_acc: 0.9085 | test_loss: 0.2308 | test_acc: 0.9138
Epoch: 48 | train_loss: 0.1511 | train_acc: 0.9391 | val_loss: 0.2503 | val_acc: 0.9073 | test_loss: 0.2298 | test_acc: 0.9108
Epoch: 49 | train_loss: 0.1506 | train_acc: 0.9395 | val_loss: 0.2726 | val_acc: 0.9023 | test_loss: 0.2424 | test_acc: 0.9115
Epoch: 50 | train_loss: 0.1490 | train_acc: 0.9408 | val_loss: 0.2555 | val_acc: 0.9040 | test_loss: 0.2331 | test_acc: 0.9100
[INFO] Saving model to: /var/scratch/ase347/DeepSummit/checkpoints/saint_epochs_50_lr_1e-5_depth_3_dropout_5e-1_decay_1e-1.pth
