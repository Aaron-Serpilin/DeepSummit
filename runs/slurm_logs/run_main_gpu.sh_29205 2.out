torch version: 2.0.1
torchvision version: 0.15.2
mlxtend version: 0.23.4
numpy version: 2.0.1
Device is: cpu

[INFO] Data splits already exist under data/himalayas_data. Skipping split.
Tabular train dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f87d8c77a00>
Tabular val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f87d8328fd0>
Tabular test dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f87d8329090>

[INFO] Created SummaryWriter, saving to: runs/saint_runs/epochs_50_lr_1e-5_depth_2_dropout_25e-2/2025-06-11--01:24:46
Epoch: 1 | train_loss: 0.2530 | train_acc: 0.8987 | val_loss: 0.2340 | val_acc: 0.9015 | test_loss: 0.2188 | test_acc: 0.9111
Epoch: 2 | train_loss: 0.1943 | train_acc: 0.9229 | val_loss: 0.2175 | val_acc: 0.9141 | test_loss: 0.2150 | test_acc: 0.9149
Epoch: 3 | train_loss: 0.1889 | train_acc: 0.9238 | val_loss: 0.2274 | val_acc: 0.9015 | test_loss: 0.2070 | test_acc: 0.9185
Epoch: 4 | train_loss: 0.1846 | train_acc: 0.9251 | val_loss: 0.2326 | val_acc: 0.8998 | test_loss: 0.2033 | test_acc: 0.9204
Epoch: 5 | train_loss: 0.1825 | train_acc: 0.9261 | val_loss: 0.2160 | val_acc: 0.9104 | test_loss: 0.2062 | test_acc: 0.9175
Epoch: 6 | train_loss: 0.1793 | train_acc: 0.9284 | val_loss: 0.2158 | val_acc: 0.9127 | test_loss: 0.2035 | test_acc: 0.9196
Epoch: 7 | train_loss: 0.1779 | train_acc: 0.9288 | val_loss: 0.2147 | val_acc: 0.9106 | test_loss: 0.2024 | test_acc: 0.9211
Epoch: 8 | train_loss: 0.1770 | train_acc: 0.9289 | val_loss: 0.2219 | val_acc: 0.9081 | test_loss: 0.2017 | test_acc: 0.9226
Epoch: 9 | train_loss: 0.1757 | train_acc: 0.9295 | val_loss: 0.2328 | val_acc: 0.9042 | test_loss: 0.1996 | test_acc: 0.9230
Epoch: 10 | train_loss: 0.1750 | train_acc: 0.9309 | val_loss: 0.2267 | val_acc: 0.9064 | test_loss: 0.1981 | test_acc: 0.9238
Epoch: 11 | train_loss: 0.1736 | train_acc: 0.9307 | val_loss: 0.2164 | val_acc: 0.9102 | test_loss: 0.2014 | test_acc: 0.9207
Epoch: 12 | train_loss: 0.1730 | train_acc: 0.9306 | val_loss: 0.2401 | val_acc: 0.8995 | test_loss: 0.2038 | test_acc: 0.9206
Epoch: 13 | train_loss: 0.1725 | train_acc: 0.9312 | val_loss: 0.2426 | val_acc: 0.9036 | test_loss: 0.2019 | test_acc: 0.9200
Epoch: 14 | train_loss: 0.1712 | train_acc: 0.9320 | val_loss: 0.2372 | val_acc: 0.9060 | test_loss: 0.2021 | test_acc: 0.9226
Epoch: 15 | train_loss: 0.1702 | train_acc: 0.9309 | val_loss: 0.2253 | val_acc: 0.9092 | test_loss: 0.1978 | test_acc: 0.9224
Epoch: 16 | train_loss: 0.1694 | train_acc: 0.9325 | val_loss: 0.2247 | val_acc: 0.9119 | test_loss: 0.2025 | test_acc: 0.9209
Epoch: 17 | train_loss: 0.1677 | train_acc: 0.9326 | val_loss: 0.2211 | val_acc: 0.9121 | test_loss: 0.2023 | test_acc: 0.9207
Epoch: 18 | train_loss: 0.1669 | train_acc: 0.9332 | val_loss: 0.2348 | val_acc: 0.9023 | test_loss: 0.2032 | test_acc: 0.9207
Epoch: 19 | train_loss: 0.1666 | train_acc: 0.9331 | val_loss: 0.2316 | val_acc: 0.9057 | test_loss: 0.2031 | test_acc: 0.9196
Epoch: 20 | train_loss: 0.1655 | train_acc: 0.9336 | val_loss: 0.2281 | val_acc: 0.9094 | test_loss: 0.2006 | test_acc: 0.9217
Epoch: 21 | train_loss: 0.1643 | train_acc: 0.9343 | val_loss: 0.2317 | val_acc: 0.9100 | test_loss: 0.2057 | test_acc: 0.9189
Epoch: 22 | train_loss: 0.1635 | train_acc: 0.9342 | val_loss: 0.2379 | val_acc: 0.9066 | test_loss: 0.2041 | test_acc: 0.9200
Epoch: 23 | train_loss: 0.1630 | train_acc: 0.9345 | val_loss: 0.2340 | val_acc: 0.9077 | test_loss: 0.2063 | test_acc: 0.9207
Epoch: 24 | train_loss: 0.1618 | train_acc: 0.9350 | val_loss: 0.2378 | val_acc: 0.9092 | test_loss: 0.2059 | test_acc: 0.9198
Epoch: 25 | train_loss: 0.1607 | train_acc: 0.9354 | val_loss: 0.2607 | val_acc: 0.8993 | test_loss: 0.2099 | test_acc: 0.9160
Epoch: 26 | train_loss: 0.1597 | train_acc: 0.9357 | val_loss: 0.2713 | val_acc: 0.8993 | test_loss: 0.2184 | test_acc: 0.9177
Epoch: 27 | train_loss: 0.1587 | train_acc: 0.9361 | val_loss: 0.2789 | val_acc: 0.8895 | test_loss: 0.2161 | test_acc: 0.9145
Epoch: 28 | train_loss: 0.1579 | train_acc: 0.9371 | val_loss: 0.2622 | val_acc: 0.8991 | test_loss: 0.2120 | test_acc: 0.9153
Epoch: 29 | train_loss: 0.1564 | train_acc: 0.9375 | val_loss: 0.2454 | val_acc: 0.9027 | test_loss: 0.2079 | test_acc: 0.9179
Epoch: 30 | train_loss: 0.1556 | train_acc: 0.9373 | val_loss: 0.2663 | val_acc: 0.9013 | test_loss: 0.2153 | test_acc: 0.9179
Epoch: 31 | train_loss: 0.1545 | train_acc: 0.9384 | val_loss: 0.2468 | val_acc: 0.9060 | test_loss: 0.2140 | test_acc: 0.9170
Epoch: 32 | train_loss: 0.1537 | train_acc: 0.9386 | val_loss: 0.2700 | val_acc: 0.8998 | test_loss: 0.2157 | test_acc: 0.9162
Epoch: 33 | train_loss: 0.1526 | train_acc: 0.9386 | val_loss: 0.2706 | val_acc: 0.9009 | test_loss: 0.2216 | test_acc: 0.9160
Epoch: 34 | train_loss: 0.1513 | train_acc: 0.9394 | val_loss: 0.2549 | val_acc: 0.9040 | test_loss: 0.2156 | test_acc: 0.9172
Epoch: 35 | train_loss: 0.1504 | train_acc: 0.9392 | val_loss: 0.2737 | val_acc: 0.9015 | test_loss: 0.2205 | test_acc: 0.9149
Epoch: 36 | train_loss: 0.1491 | train_acc: 0.9406 | val_loss: 0.2552 | val_acc: 0.9047 | test_loss: 0.2219 | test_acc: 0.9151
Epoch: 37 | train_loss: 0.1480 | train_acc: 0.9399 | val_loss: 0.3075 | val_acc: 0.8940 | test_loss: 0.2359 | test_acc: 0.9145
Epoch: 38 | train_loss: 0.1476 | train_acc: 0.9413 | val_loss: 0.3009 | val_acc: 0.9012 | test_loss: 0.2352 | test_acc: 0.9160
Epoch: 39 | train_loss: 0.1464 | train_acc: 0.9412 | val_loss: 0.2912 | val_acc: 0.8959 | test_loss: 0.2304 | test_acc: 0.9134
Epoch: 40 | train_loss: 0.1448 | train_acc: 0.9416 | val_loss: 0.3150 | val_acc: 0.8962 | test_loss: 0.2445 | test_acc: 0.9140
Epoch: 41 | train_loss: 0.1445 | train_acc: 0.9420 | val_loss: 0.3267 | val_acc: 0.8923 | test_loss: 0.2442 | test_acc: 0.9132
Epoch: 42 | train_loss: 0.1439 | train_acc: 0.9436 | val_loss: 0.3141 | val_acc: 0.8919 | test_loss: 0.2384 | test_acc: 0.9106
Epoch: 43 | train_loss: 0.1426 | train_acc: 0.9428 | val_loss: 0.2934 | val_acc: 0.9044 | test_loss: 0.2357 | test_acc: 0.9155
Epoch: 44 | train_loss: 0.1415 | train_acc: 0.9436 | val_loss: 0.3067 | val_acc: 0.8970 | test_loss: 0.2365 | test_acc: 0.9145
Epoch: 45 | train_loss: 0.1407 | train_acc: 0.9439 | val_loss: 0.3119 | val_acc: 0.8955 | test_loss: 0.2350 | test_acc: 0.9151
Epoch: 46 | train_loss: 0.1395 | train_acc: 0.9437 | val_loss: 0.3527 | val_acc: 0.8996 | test_loss: 0.2480 | test_acc: 0.9160
Epoch: 47 | train_loss: 0.1383 | train_acc: 0.9451 | val_loss: 0.3550 | val_acc: 0.8885 | test_loss: 0.2543 | test_acc: 0.9121
Epoch: 48 | train_loss: 0.1376 | train_acc: 0.9453 | val_loss: 0.3451 | val_acc: 0.8953 | test_loss: 0.2578 | test_acc: 0.9155
Epoch: 49 | train_loss: 0.1366 | train_acc: 0.9460 | val_loss: 0.3354 | val_acc: 0.8944 | test_loss: 0.2550 | test_acc: 0.9128
Epoch: 50 | train_loss: 0.1359 | train_acc: 0.9457 | val_loss: 0.3064 | val_acc: 0.8970 | test_loss: 0.2446 | test_acc: 0.9125
[INFO] Saving model to: /var/scratch/ase347/DeepSummit/checkpoints/saint_epochs_50_lr_1e-5_depth_2_dropout_25e-2.pth
